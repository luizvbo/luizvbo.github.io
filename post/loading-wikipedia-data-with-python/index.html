<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Luiz Otavio Vilas Boas Oliveira">

  
  
  
    
  
  <meta name="description" content="Wikipedia is the largest and most popular general reference work on the World Wide Web, and is one of the most popular websites ranked by Alexa as of January 2020.
As of February 2020, there are 6,016,720 articles in the English Wikipedia containing over 3.5 billion words. There are a lot of information about the amount of data in Wikipedia that can be found in this article .
 Content page count of the English-language Wikipedia from the beginning to 2019-03-21">

  
  <link rel="alternate" hreflang="en-us" href="/post/loading-wikipedia-data-with-python/">

  


  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-135928539-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  gtag('js', new Date());
  gtag('config', 'UA-135928539-1');
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/loading-wikipedia-data-with-python/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Luiz Otavio V. B. Oliveira">
  <meta property="og:url" content="/post/loading-wikipedia-data-with-python/">
  <meta property="og:title" content="Loading Wikipedia articles (EN) with Python | Luiz Otavio V. B. Oliveira">
  <meta property="og:description" content="Wikipedia is the largest and most popular general reference work on the World Wide Web, and is one of the most popular websites ranked by Alexa as of January 2020.
As of February 2020, there are 6,016,720 articles in the English Wikipedia containing over 3.5 billion words. There are a lot of information about the amount of data in Wikipedia that can be found in this article .
 Content page count of the English-language Wikipedia from the beginning to 2019-03-21"><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-03-04T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-03-04T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/loading-wikipedia-data-with-python/"
  },
  "headline": "Loading Wikipedia articles (EN) with Python",
  
  "datePublished": "2020-03-04T00:00:00Z",
  "dateModified": "2020-03-04T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Luiz Otavio Vilas Boas Oliveira"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Luiz Otavio V. B. Oliveira",
    "logo": {
      "@type": "ImageObject",
      "url": "img//"
    }
  },
  "description": "Wikipedia is the largest and most popular general reference work on the World Wide Web, and is one of the most popular websites ranked by Alexa as of January 2020.\nAs of February 2020, there are 6,016,720 articles in the English Wikipedia containing over 3.5 billion words. There are a lot of information about the amount of data in Wikipedia that can be found in this article .\n Content page count of the English-language Wikipedia from the beginning to 2019-03-21"
}
</script>

  

  


  


  





  <title>Loading Wikipedia articles (EN) with Python | Luiz Otavio V. B. Oliveira</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Luiz Otavio V. B. Oliveira</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Luiz Otavio V. B. Oliveira</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience"><span>Experience</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#accomplishments"><span>AccomplishÂ­ments</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Loading Wikipedia articles (EN) with Python</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Mar 4, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Wikipedia is the largest and most popular general reference work on the World Wide Web, and is one of the most popular websites ranked by Alexa as of January 2020.</p>
<p>As of February 2020, there are 6,016,720 articles in the English Wikipedia containing over 3.5 billion words. There are a lot of information about the amount of data in Wikipedia that can be found in 
<a href="https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia" target="_blank" rel="noopener">this article</a>
.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/1/19/Size_of_English_Wikipedia_graph_2019-03-01.png" alt="Content page count of the English-language Wikipedia from the beginning to 2019-03-21"></p>
<blockquote>
<p>Content page count of the English-language Wikipedia from the beginning to 2019-03-21</p>
</blockquote>
<p><strong>This article is going to show you how to download the whole Wikipedia (in English) and load the data in Python.</strong></p>
<p>You can dowload the original Notebook from my 
<a href="https://github.com/luizvbo/wikidata/blob/master/wikipedia_store.ipynb" target="_blank" rel="noopener">GitHub repository</a>
.</p>
<h2 id="downloading-the-data">Downloading the Data</h2>
<p>The fastest way of getting Wikipedia is though torrent. Download the last version of the data dump from this 
<a href="https://meta.wikimedia.org/wiki/Data_dump_torrents#English_Wikipedia" target="_blank" rel="noopener">page</a>
. You should download the newest version of the file <code>enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2</code>, where <code>YYYYMMDD</code> is the date of the dump.</p>
<p>The MD5 for the file can be found in the link <a href="http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/md5sums-enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2.txt">http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/md5sums-enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2.txt</a> where <code>YYYYMMDD</code> should be replace by the date of the dump.</p>
<p>After downloading the articles <strong>bz2</strong> file, we need to download the list on indices for the articles from <a href="http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2">http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2</a>. Again, replace <code>YYYYMMDD</code> by the date of the dump.</p>
<p>After downloading the files you should extract <strong>only the index</strong> file. On Linux, we can use <code>lbzip2</code> to uncompress the file using multiple CPUs, speeding up the process. In the terminal, in the file folder type:</p>
<pre><code class="language-bash">$ lbzip2 -d enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2
</code></pre>
<h2 id="loading-the-data">Loading the Data</h2>
<p>Now is where the things start to become interesting. Since the file is too large to fit in memory, we are going to load it iteratively.</p>
<pre><code class="language-python">import pyarrow.parquet as pq
import pyarrow as pa
import pandas as pd
import numpy as np
import itertools
import os
import io

from multiprocessing import Pool
from tqdm import tqdm
from lxml import etree
from bz2 import BZ2Decompressor
from typing import (
    List, Generator
)

# Path to the bz2 files with Wikipedia data
path_articles = 'enwiki-20200201-pages-articles-multistream.xml.bz2'
# Path to the index list from Wikipedia
path_index = 'enwiki-20200201-pages-articles-multistream-index.txt'
# Path to our cached version (for offsets)
path_index_clean = 'enwiki-20200201-pages-articles-multistream-index_clean.txt'
# Path to the output parquet file
path_wiki_parquet = 'wikipedia.parquet'
# Number of processors to be used during processing
n_processors = 16
# Number of blocks of pages to be processed per iteration per processor
n_parallel_blocks = 15
</code></pre>
<p>The multistream dump file contains multiple bz2 &lsquo;streams&rsquo; (bz2 header, body, footer) concatenated together into one file, in contrast to the vanilla file which contains one stream. Each separate &lsquo;stream&rsquo; (or really, file) in the multistream dump contains 100 pages, except possibly the last one. The multistream file allows you to get an article from the archive without unpacking the whole thing.</p>
<p>The index file contains the full list of articles. The first field of this index is the number of bytes to seek into the compressed archive pages-articles-multistream.xml.bz2, the second is the article ID, the third the article title. A colon (<code>:</code>) is used to separate fields.</p>
<p>Since we would like to extract all the articles from wikipedia, we don&rsquo;t have to keep track of titles and IDs, only the offsets. Thus, we read the offsets and store them into a new file.</p>
<pre><code class="language-python">def get_page_offsets(path_index: str, path_index_clean: str) -&gt; List[int]:
    &quot;&quot;&quot;Get page offsets from wikipedia file or cached version

    Wikipedia provide an index file containing the list of articles with their
    respective id and offset from the start of the file. Since we are
    interested only on the offsets, we read the original file, provided by
    `path_index`, extract the offsets and store in another file (defined by
    `path_index_clean`) to speed up the process

    Args:
        path_index (str): Path to the original index file provided by Wikipedia
        path_index_clean (str): Path to our version, containing only offsets

    Returns:
        List[int]: List of offsets
    &quot;&quot;&quot;
    # Get the list of offsets
    # If our new offset file was not created, it gets the information
    # from the index file
    if not os.path.isfile(path_index_clean):
        # Read the byte offsets from the index file
        page_offset = []
        last_offset = None
        with open(path_index, 'r') as f:
            for line in tqdm(f):
                offset = line.split(':', 1)[0]
                if last_offset != offset:
                    last_offset = offset
                    page_offset.append(int(offset))

        with open(path_index_clean, 'w') as f:
            f.write(','.join([str(i) for i in page_offset]))
    else:
        with open(path_index_clean, 'r') as f:
            page_offset = [int(idx) for idx in f.read().split(',')]

    return page_offset
</code></pre>
<h2 id="parsing-the-data">Parsing the data</h2>
<p>In order to parse the files, we need to open the bz2 file containing the articles, read blocks of bytes, according to the offsets defined above, and then uncompress these blocks.</p>
<p>The generator <code>get_bz2_byte_str</code> reads the blocks sequentially, following the list of offsets. And the function <code>get_articles</code> is used to convert each byte string into a pandas data frame containing the index, title and content of the article.</p>
<p><strong>Note</strong>: The XML structure can be found on this 
<a href="https://meta.wikimedia.org/wiki/Data_dumps/Dump_format" target="_blank" rel="noopener">page</a>
.</p>
<pre><code class="language-python">def get_bz2_byte_str(path_articles: str,
                     offset_list: List[int]) -&gt; Generator[bytes, None, None]:
    &quot;&quot;&quot;Read the multistream bz2 file using the offset list

    The offset list defines where the bz2 (sub)file starts and ends

    Args:
        path_articles (str): Path to the bz2 file containing the Wikipedia
            articles.
        offset_list (List[int]): List of byte offsets

    Yields:
        bytes: String of bytes corresponding to a set of articles compressed
    &quot;&quot;&quot;
    with open(path_articles, &quot;rb&quot;) as f:
        last_offset = offset_list[0]
        # Drop the data before the offset
        f.read(last_offset)
        for next_offset in offset_list[1:]:
            offset = next_offset - last_offset
            last_offset = next_offset
            yield f.read(offset)
</code></pre>
<pre><code class="language-python">def get_articles(byte_string_compressed: bytes) -&gt; pd.DataFrame:
    &quot;&quot;&quot;Get a dataframe containing the set of articles from a bz2

    Args:
        byte_string_compressed (bytes): Byte string corresponding to the bz2
            stream

    Returns:
        pd.DataFrame: Dataframe with columns title and article
    &quot;&quot;&quot;
    def _get_text(list_xml_el):
        &quot;&quot;&quot;Return the list of content for a list of xml_elements&quot;&quot;&quot;
        return [el.text for el in list_xml_el]

    def _get_id(list_xml_el):
        &quot;&quot;&quot;Return the list of id's for a list of xml_elements&quot;&quot;&quot;
        return [int(el.text) for el in list_xml_el]

    bz2d = BZ2Decompressor()
    byte_string = bz2d.decompress(byte_string_compressed)
    doc = etree.parse(io.BytesIO(b'&lt;root&gt; ' + byte_string + b' &lt;/root&gt;'))

    col_id = _get_id(doc.xpath('*/id'))
    col_title = _get_text(doc.xpath('*/title'))
    col_article = _get_text(doc.xpath('*/revision/text'))

    return pd.DataFrame([col_title, col_article], columns=col_id, index=['title', 'article']).T
</code></pre>
<h2 id="reading-and-storing-in-parquet-files">Reading and storing in parquet files</h2>
<p>We read the blocks of the bz2 file, extract the data and write to parquet files. In order to speed up the process we use a queue to store the blocks of bytes that are processed in parallel.</p>
<pre><code class="language-python">def chunks(input_list: List, chunk_size: int) -&gt; Generator[List, None, None]:
    &quot;&quot;&quot;Split a list into chunks of size `chunk_size`

    Args:
        input_list (List): Input list
        chunk_size (int): Size of the chunks. Note that the last chunk may have
            less than `chunk_size` elements

    Yields:
        Generator[List, None, None]: Sublist of size `chunk_size`
    &quot;&quot;&quot;

    # For item i in a range that is a length of l,
    for i in range(0, len(input_list), chunk_size):
        # Create an index range for l of n items:
        yield input_list[i:i+chunk_size]


def _process_parallel(list_bytes: List[bytes]) -&gt; None:
    &quot;&quot;&quot;Process a subset of the byte chunks from the original dump file

    Args:
        list_bytes (List[bytes]): List of byte strings (chunks from the
            original file)
    &quot;&quot;&quot;

    df = pd.concat([get_articles(article) for article in list_bytes])

    # Create a parquet table from your dataframe
    table = pa.Table.from_pandas(df)

    # Write direct to your parquet file
    pq.write_to_dataset(table, root_path=path_wiki_parquet, compression='BROTLI')

    # Clear the data tables
    del df
    del table
</code></pre>
<p>The code bellow stores each block of 100 articles in a new parquet file. It allows you to load a subset of the full dump or work using 
<a href="https://github.com/dask/dask" target="_blank" rel="noopener">Dask</a>
, 
<a href="https://github.com/modin-project/modin" target="_blank" rel="noopener">Modin</a>
 or 
<a href="https://spark.apache.org/docs/latest/api/python/index.html" target="_blank" rel="noopener">pySpark</a>
.</p>
<p>I am using 
<a href="https://github.com/google/brotli" target="_blank" rel="noopener">BROTLI</a>
 to reduce the amount of space used by the extracted data.</p>
<blockquote>
<p>Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods.</p>
</blockquote>
<pre><code class="language-python">queue = []
page_offset = get_page_offsets(path_index, path_index_clean)
# Read the file sequentially
for bit_str in tqdm(get_bz2_byte_str(path_articles, page_offset), total=len(page_offset)):
    # Feed the queue
    if len(queue) &lt; n_processors * n_parallel_blocks:
        queue.append(bit_str)

    # Decompress and extract the infomation in parallel
    else:
        with Pool(processes=n_processors) as pool:
            tuple(pool.imap_unordered(_process_parallel, chunks(queue, n_parallel_blocks)))
        # Clean the queue
        for el in queue:
            del el
        queue.clear()
# Run one last time
with Pool(processes=n_processors) as pool:
    tuple(pool.imap_unordered(_process_parallel, chunks(queue, n_parallel_blocks)))
# Clean the queue
for el in queue:
    del el
queue.clear()
</code></pre>
<h2 id="next-steps">Next Steps</h2>
<p>Done! Now, we can load the data from parquet.</p>
<p>However, we still have to parse the data from the articles, since there are some markups used to Wikipedia for citations, info boxes, categories and so on. We will deal with these in the next artile.</p>

    </div>

    







<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/loading-wikipedia-data-with-python/&amp;text=Loading%20Wikipedia%20articles%20%28EN%29%20with%20Python" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/loading-wikipedia-data-with-python/&amp;t=Loading%20Wikipedia%20articles%20%28EN%29%20with%20Python" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Loading%20Wikipedia%20articles%20%28EN%29%20with%20Python&amp;body=/post/loading-wikipedia-data-with-python/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/loading-wikipedia-data-with-python/&amp;title=Loading%20Wikipedia%20articles%20%28EN%29%20with%20Python" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Loading%20Wikipedia%20articles%20%28EN%29%20with%20Python%20/post/loading-wikipedia-data-with-python/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/loading-wikipedia-data-with-python/&amp;title=Loading%20Wikipedia%20articles%20%28EN%29%20with%20Python" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hua37112fd29e767f6dab39c2dbbc27d16_76892_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Luiz Otavio Vilas Boas Oliveira</a></h5>
      <h6 class="card-subtitle">Data Scientist</h6>
      <p class="card-text">A Brazilian in the Netherlands solving problems using data.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/luiz.vbo@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/luizvbo" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=c6bK3vUAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/luizvbo" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>




<section id="comments">
  
    <div id="commento"></div>

<script src="https://cdn.commento.io/js/commento.js" defer></script>

  
</section>






  
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.0630fec5958cb075a5a38f042b3ddde6.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
