<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Luiz Otavio Vilas Boas Oliveira</title>
    <link>https://luizvbo.github.io/post/</link>
      <atom:link href="https://luizvbo.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 11 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>https://luizvbo.github.io/post/</link>
    </image>
    
    <item>
      <title>Predicting COVID-19 Deaths by Similarity</title>
      <link>https://luizvbo.github.io/post/corona-linear-regression/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      <guid>https://luizvbo.github.io/post/corona-linear-regression/</guid>
      <description>&lt;p&gt;I have been seen some discussion about epidemiologic models to predict the number of cases and deaths by COVID-19, which made me give some thought about the subject.&lt;/p&gt;
&lt;p&gt;Since we have countries in different stages of the pandemic, my hypothesis was that we could use information from countries in advanced stages to predict information for countries at the beginning of the crisis.&lt;/p&gt;
&lt;p&gt;Considering the number of deaths by COVID-19 registered, we can align the data for all countries, such that the day the first death by COVID-19 was registered for each country coincides with the origin.&lt;/p&gt;
&lt;p&gt;With the data shifted, we can compute the correlation between each pair of countries and then find those pairs with high correlation. We can fit a linear regression to each of these pairs considering the country with more data as the independent variable and the one with fewer data as the response variable.&lt;/p&gt;
&lt;p&gt;We can then use the data for the country with more data to predict the number of deaths for the other country. For, instance, for Brazil, the 8 highest correlated countries are Canada, USA, Egypt, Japan, Argentina, UK, Italy and Iran (all of them with correlation &amp;gt; 0.95).&lt;/p&gt;
&lt;p&gt;I used the data provided by the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) in their 
&lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;The code for this notebook can be downloaded from 
&lt;a href=&#34;https://github.com/luizvbo/notebooks/blob/master/covid_linear_reg.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import Tuple, Dict
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from tqdm import tqdm
from functools import partial
from glob import glob
from multiprocessing import Pool
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# URL to the CSSE repository
url_covid_death = (&amp;quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/&amp;quot;
                   &amp;quot;master/csse_covid_19_data/csse_covid_19_time_series/&amp;quot;
                   &amp;quot;time_series_covid19_deaths_global.csv&amp;quot;)
# URL to the population data from Worldbank
url_pop = (&amp;quot;http://api.worldbank.org/v2/en/indicator/&amp;quot;
           &amp;quot;SP.POP.TOTL?downloadformat=csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_same_origin(df: pd.DataFrame) -&amp;gt; pd.DataFrame:
    &amp;quot;&amp;quot;&amp;quot;Move first case to the origing

    Args:
        df (pd.DataFrame): Input data frame, where each column corresponds to
        one country. It should be the output of the function `set_index`.

    Returns:
        pd.DataFrame: Data frame with every column shifted up
    &amp;quot;&amp;quot;&amp;quot;
    n_days = df.shape[0]

    def _pad_days(s):
        s = s.astype(float)
        s_pad = s[s.cumsum() != 0]
        return np.pad(s_pad, (0, n_days-s_pad.shape[0]),
                      &#39;constant&#39;, constant_values=np.nan)

    df = (
        df.apply(_pad_days, raw=True)
        .reset_index(drop=True)
    ).dropna(how=&#39;all&#39;)

    return df


def set_index(df: pd.DataFrame) -&amp;gt; pd.DataFrame:
    &amp;quot;&amp;quot;&amp;quot;Set the index for the data frame using the date

    Args:
        df (pd.DataFrame): Data frame obtained from John Hopkins repo

    Returns:
        pd.DataFrame: Preprocessed data
    &amp;quot;&amp;quot;&amp;quot;
    # Set region, country, lat and long as index
    index = pd.MultiIndex.from_frame(df.iloc[:, :4])
    # Set the index and transpose
    df = df.iloc[:, 4:].set_index(index).T
    # Set date as index
    return df.set_index(pd.to_datetime(df.index, dayfirst=False))


def compute_lr(country: Tuple[int, str], df_covid: pd.DataFrame,
               min_diff: int = 7) -&amp;gt; Dict[Tuple[str, str], Dict]:
    &amp;quot;&amp;quot;&amp;quot;Fit the logistic regression for each pair of countries

    Args:
        country (Tuple[int, str]): Tuple with the index of the
            country in the column list and the country
        df_covid (pd.DataFrame): Data frame containing the data shifted to
            the origin.
        min_diff (int, optional): Minimum difference in the number of days to
            consider the country pair. Defaults to 7.

    Returns:
        Dict[Tuple[str, str], Dict]: Dictionary where the keys are the pairs
            of countries and the value is a data dictionary with the data
            computued.
    &amp;quot;&amp;quot;&amp;quot;
    i, col_1 = country
    results = {}

    for col_2 in df_covid.columns[i+1:]:

        x = df_covid[col_1].dropna().values
        y = df_covid[col_2].dropna().values

        # Keep the largest array in x
        if x.shape[0] &amp;lt; y.shape[0]:
            x, y = y, x
            x_label, y_label = col_2, col_1
        else:
            x_label, y_label = col_1, col_2

        x, y = x.reshape(-1, 1), y.reshape(-1, 1)

        if x.shape[0] - y.shape[0] &amp;gt; min_diff:
            lr = LinearRegression()
            # The weights increase linearly from 1 to 2
            weights = np.linspace(1, 1, y.shape[0])
            lr.fit(x[:y.shape[0]], y, weights)

            results[(x_label, y_label)] = dict(
                lr_model=lr,
                r_score=lr.score(x[:y.shape[0]], y),
                predicted=lr.predict(x),
                x=x, y=y,
            )

    return results


def plot_candidates(df_candidates: pd.DataFrame,
                    nrows: int = 4, ncols: int = 2,
                    over_days: bool = True,
                    figsize: Tuple[int, int] = (12, 15)):
    &amp;quot;&amp;quot;&amp;quot;Plot the regression for pairs of countries

    Args:
        df_candidates (pd.DataFrame): Data frame with pairs of countries with
            the linear regression data

        nrows (int, optional): Number of rows to plot. Defaults to 4.
        ncols (int, optional): NBumber of columns to plot. Defaults to 2.
        over_days (bool, optional): If True, plot the data over days.
            Otherwise, plot one country agains the other. Defaults to True.
        figsize (Tuple[int, int], optional): Size of the resulting fiture.
            Defaults to (12, 15).

    Returns:
        [type]: Figure object
    &amp;quot;&amp;quot;&amp;quot;
    fig, axs = plt.subplots(nrows, ncols)
    df_ = df_candidates.head(nrows * ncols)
    for (i, row), ax in zip(df_.iterrows(), axs.flatten()):
        if over_days:
            ax.plot(row[&#39;x&#39;], label=i[0])
            ax.plot(row[&#39;y&#39;], label=i[1])
            ax.plot(row[&#39;predicted&#39;], &#39;--&#39;, label=f&amp;quot;{i[1]} (predicted)&amp;quot;)
            ax.set_xlabel(&amp;quot;Days since the first death by COVID-19&amp;quot;)
            ax.set_ylabel(&amp;quot;Number of deaths&amp;quot;)
        else:
            ax.plot(row[&#39;x&#39;][:row[&#39;y&#39;].shape[0]], row[&#39;y&#39;], label=&#39;True value&#39;)
            ax.plot(row[&#39;x&#39;], row[&#39;predicted&#39;], &#39;--&#39;, label=&#39;Predicted&#39;)
            ax.set_xlabel(i[0])
            ax.set_ylabel(i[1])

        ax.grid(True)
        ax.legend(title=&amp;quot;$r^2={:.3f}$&amp;quot;.format(row[&#39;r_score&#39;]),
                  loc=&#39;upper left&#39;)

    fig.set_size_inches(*figsize)
    return fig
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Getting the population information
!wget {url_pop} -O pop.zip
!unzip -o pop.zip -d pop_csv
# Load the population file
df_pop_ = pd.read_csv(glob(&#39;pop_csv/API_SP.POP.TOTL*.csv&#39;)[0],  skiprows=4)
df_pop = df_pop_[[&#39;Country Name&#39;, &#39;2018&#39;]].set_index(&#39;Country Name&#39;)
# Delete the files downloaded
!rm -r pop_csv
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;--2020-05-11 22:03:17--  http://api.worldbank.org/v2/en/indicator/SP.POP.TOTL?downloadformat=csv
Resolving api.worldbank.org (api.worldbank.org)... 34.237.118.134
Connecting to api.worldbank.org (api.worldbank.org)|34.237.118.134|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 78803 (77K) [application/zip]
Saving to: ‘pop.zip’

pop.zip             100%[===================&amp;gt;]  76.96K   152KB/s    in 0.5s

2020-05-11 22:03:19 (152 KB/s) - ‘pop.zip’ saved [78803/78803]

Archive:  pop.zip
  inflating: pop_csv/Metadata_Indicator_API_SP.POP.TOTL_DS2_en_csv_v2_988606.csv
  inflating: pop_csv/API_SP.POP.TOTL_DS2_en_csv_v2_988606.csv
  inflating: pop_csv/Metadata_Country_API_SP.POP.TOTL_DS2_en_csv_v2_988606.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Loading the data for the number of Deaths
df_death = pd.read_csv(url_covid_death)

df = set_index(df_death)

# Groupy territories per country
df = df.groupby(level=1, axis=1).sum()

# # Drop all-zeros columns
df = df[df.sum()[lambda s: s &amp;gt; 0].index]

# # Shift all series to the origin (first death)
df = get_same_origin(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Ignore countries with less than 1M
countries = [c for c in df.columns if c not in
             df_pop[lambda df_: df_[&#39;2018&#39;] &amp;lt; 10**6].index]

compute_lr_parallel = partial(compute_lr, df_covid=df[countries])

with Pool(8) as pool:
    results = {}
    for res_dict in tqdm(pool.imap(compute_lr_parallel, enumerate(countries)),
                         total=df.shape[0]):
        results.update(res_dict)
df_results = pd.DataFrame.from_dict(results, orient=&#39;index&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;144it [00:02, 50.77it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;plots&#34;&gt;Plots&lt;/h1&gt;
&lt;h2 id=&#34;linear-regression-results&#34;&gt;Linear Regression Results&lt;/h2&gt;
&lt;p&gt;The first set of figures shows the output for the linear regression, considering the number of deaths from Brazil as the response variable (y-axis) and each of the other countries as the dependent variable (x-axis).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_brazil = df_results[lambda df: (df.index.get_level_values(1) == &#39;Brazil&#39;)].sort_values(&#39;r_score&#39;, ascending=False)
fig1 = plot_candidates(df_brazil, over_days=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;covid_linear_reg_8_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;predictions&#34;&gt;Predictions&lt;/h2&gt;
&lt;p&gt;The second set shows the prediction, for the number of deaths in Brazil, according to the line fitted to each of the other countries. The number of days in future is limited by the data available for the other country.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig2 = plot_candidates(df_, over_days=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;covid_linear_reg_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing the number of COVID-19 confirmed cases with Plotly</title>
      <link>https://luizvbo.github.io/post/coronavirus-confirmed/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://luizvbo.github.io/post/coronavirus-confirmed/</guid>
      <description>&lt;p&gt;I would like to see: &lt;strong&gt;The evolution over time of the number of cases/deaths/recovery patients per country&lt;/strong&gt;. Assuming that others are also interested in this information, I am sharing the plots I made with &lt;code&gt;plotly&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a 
&lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository with daily updated information about the Corona Virus&lt;/a&gt;
. They also provide a dashboard to interact with the data (however, even their dashboard doesn’t provide the plot I was looking for).&lt;/p&gt;
&lt;p&gt;You are more than welcome to modify the notebook to predict the number of cases and do other types of analysis. The notebook can be found in my &lt;strong&gt;
&lt;a href=&#34;https://github.com/luizvbo/notebooks/blob/master/corona-plots-plotly.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt;
&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The data is split into three plots:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;../coronavirus-confirmed&#34;&gt;COVID-19 confirmed cases&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;../coronavirus-death&#34;&gt;COVID-19 deaths&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;../coronavirus-recovered&#34;&gt;COVID-19 recovered cases&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bellow, you can see the data for all the &lt;strong&gt;confirmed cases&lt;/strong&gt; (total and per day).&lt;/p&gt;
&lt;p&gt;You can select countries to compare the data. If you are having problems to visualise the plot, you can try the 
&lt;a href=&#34;../coronavirus-confirmed/plot-confirmed.html&#34;&gt;clean html version from this link&lt;/a&gt;
&lt;/p&gt;
&lt;iframe id=&#34;igraph&#34; scrolling=&#34;no&#34; style=&#34;border:none;&#34;
        seamless=&#34;seamless&#34; src=&#34;plot-confirmed.html&#34; height=&#34;800px&#34; width=&#34;100%&#34;&gt;
&lt;/iframe&gt;&#39;
&lt;h1 id=&#34;data-shifted-to-origin&#34;&gt;Data Shifted to Origin&lt;/h1&gt;
&lt;p&gt;I have been seeing plots where they compare the number of new cases/deaths among different countries by moving the origin of the plots the day of the first case/death. Thus, I did the same for the plot above: all the data points are shifted in the horizontal axis such that the origin corresponds to first case/death for that country:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The data IS NOT normalised by the population size.&lt;/p&gt;
&lt;iframe id=&#34;igraph&#34; scrolling=&#34;no&#34; style=&#34;border:none;&#34;
        seamless=&#34;seamless&#34; src=&#34;plot-confirmed-origin.html&#34; height=&#34;800px&#34; width=&#34;100%&#34;&gt;
&lt;/iframe&gt;&#39;</description>
    </item>
    
    <item>
      <title>Visualizing the number of COVID-19 deaths with Plotly</title>
      <link>https://luizvbo.github.io/post/coronavirus-death/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://luizvbo.github.io/post/coronavirus-death/</guid>
      <description>&lt;p&gt;I would like to see: &lt;strong&gt;The evolution over time of the number of cases/deaths/recovery patients per country&lt;/strong&gt;. Assuming that others are also interested in this information, I am sharing the plots I made with &lt;code&gt;plotly&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a 
&lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository with daily updated information about the Corona Virus&lt;/a&gt;
. They also provide a dashboard to interact with the data (however, even their dashboard doesn’t provide the plot I was looking for).&lt;/p&gt;
&lt;p&gt;You are more than welcome to modify the notebook to predict the number of cases and do other types of analysis. The notebook can be found in my &lt;strong&gt;
&lt;a href=&#34;https://github.com/luizvbo/notebooks/blob/master/corona-plots-plotly.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt;
&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The data is split into three plots:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;../coronavirus-confirmed&#34;&gt;COVID-19 confirmed cases&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;../coronavirus-death&#34;&gt;COVID-19 deaths&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;../coronavirus-recovered&#34;&gt;COVID-19 recovered cases&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bellow, you can see the data for the &lt;strong&gt;deaths&lt;/strong&gt; caused by COVID-19 (total and per day).&lt;/p&gt;
&lt;p&gt;You can select countries to compare the data. If you are having problems to visualise the plot, you can try the 
&lt;a href=&#34;../coronavirus-death/plot-death.html&#34;&gt;clean html version from this link&lt;/a&gt;
&lt;/p&gt;
&lt;iframe id=&#34;igraph&#34; scrolling=&#34;no&#34; style=&#34;border:none;&#34;
        seamless=&#34;seamless&#34; src=&#34;plot-death.html&#34; height=&#34;800px&#34; width=&#34;100%&#34;&gt;
&lt;/iframe&gt;&#39;
&lt;h1 id=&#34;data-shifted-to-origin&#34;&gt;Data Shifted to Origin&lt;/h1&gt;
&lt;p&gt;I have been seeing plots where they compare the number of new cases/deaths among different countries by moving the origin of the plots the day of the first case/death. Thus, I did the same for the plot above: all the data points are shifted in the horizontal axis such that the origin corresponds to first case/death for that country:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The data IS NOT normalised by the population size.&lt;/p&gt;
&lt;iframe id=&#34;igraph&#34; scrolling=&#34;no&#34; style=&#34;border:none;&#34;
        seamless=&#34;seamless&#34; src=&#34;plot-death-origin.html&#34; height=&#34;800px&#34; width=&#34;100%&#34;&gt;
&lt;/iframe&gt;&#39;</description>
    </item>
    
    <item>
      <title>Visualizing the number of COVID-19 recovered cases with Plotly</title>
      <link>https://luizvbo.github.io/post/coronavirus-recovered/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://luizvbo.github.io/post/coronavirus-recovered/</guid>
      <description>&lt;p&gt;I would like to see: &lt;strong&gt;The evolution over time of the number of cases/deaths/recovery patients per country&lt;/strong&gt;. Assuming that others are also interested in this information, I am sharing the plots I made with &lt;code&gt;plotly&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a 
&lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository with daily updated information about the Corona Virus&lt;/a&gt;
. They also provide a dashboard to interact with the data (however, even their dashboard doesn’t provide the plot I was looking for).&lt;/p&gt;
&lt;p&gt;You are more than welcome to modify the notebook to predict the number of cases and do other types of analysis. The notebook can be found in my &lt;strong&gt;
&lt;a href=&#34;https://github.com/luizvbo/notebooks/blob/master/corona-plots-plotly.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt;
&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The data is split into three plots:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;../coronavirus-confirmed&#34;&gt;COVID-19 confirmed cases&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;../coronavirus-death&#34;&gt;COVID-19 deaths&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;../coronavirus-recovered&#34;&gt;COVID-19 recovered cases&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bellow, you can see the data for all the &lt;strong&gt;recovered cases&lt;/strong&gt; (total and per day).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Starting from 25/03/2020, the JHU CSSE is not updating the number of recovered cases anymore. Thus, the plot bellow will not be updated anymore.&lt;/p&gt;
&lt;p&gt;You can select countries to compare the data. If you are having problems to visualise the plot, you can try the 
&lt;a href=&#34;../coronavirus-recovered/plot-recovered.html&#34;&gt;clean html version from this link&lt;/a&gt;
&lt;/p&gt;
&lt;iframe id=&#34;igraph&#34; scrolling=&#34;no&#34; style=&#34;border:none;&#34;
        seamless=&#34;seamless&#34; src=&#34;plot-recovered.html&#34; height=&#34;800px&#34; width=&#34;100%&#34;&gt;
&lt;/iframe&gt;&#39;
</description>
    </item>
    
    <item>
      <title>Visualizing COVID-19 infection (corona virus) over time</title>
      <link>https://luizvbo.github.io/post/coronavirus-visualization/</link>
      <pubDate>Thu, 12 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://luizvbo.github.io/post/coronavirus-visualization/</guid>
      <description>&lt;h1 id=&#34;exploring-covid-19&#34;&gt;Exploring COVID-19&lt;/h1&gt;
&lt;p&gt;The Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a 
&lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository with daily updated information about the Corona Virus&lt;/a&gt;
. They also provide a dashboard to interact with the data.&lt;/p&gt;
&lt;p&gt;However, even their dashboard doesn’t provide the information I would like to see: &lt;strong&gt;The evolution over time of the number of cases/deaths/recovery patients per country&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Thus, the idea of this notebook is to create interactive plots that we can use to monitor the evolution of the disease.&lt;/p&gt;
&lt;p&gt;You are more than welcome to modify the notebook to predict the number of cases and do other types of analysis. The notebook can be found in my &lt;strong&gt;
&lt;a href=&#34;https://github.com/luizvbo/notebooks/blob/master/corona-plots.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt;
&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;imports&#34;&gt;Imports&lt;/h2&gt;
&lt;p&gt;I am using Pandas + cufflinks to plot our data using Plotly.&lt;/p&gt;
&lt;p&gt;I am also using ipywidgets to interact with the plots. If you are using Jupyterlab, you may need to refer to the 
&lt;a href=&#34;https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;installation documention&lt;/a&gt;
.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
from IPython.display import Markdown, display
from ipywidgets import interact
from ipywidgets.widgets import (
    Dropdown, SelectionSlider, Checkbox
)
from datetime import datetime
import cufflinks as cf
&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&#34;text/javascript&#34;&gt;
window.PlotlyConfig = {MathJaxConfig: &#39;local&#39;};
if (window.MathJax) {MathJax.Hub.Config({SVG: {font: &#34;STIX-Web&#34;}});}
if (typeof require !== &#39;undefined&#39;) {
require.undef(&#34;plotly&#34;);
requirejs.config({
    paths: {
        &#39;plotly&#39;: [&#39;https://cdn.plot.ly/plotly-latest.min&#39;]
    }
});
require([&#39;plotly&#39;], function(Plotly) {
    window._Plotly = Plotly;
});
}
&lt;/script&gt;
&lt;h2 id=&#34;getting-the-data&#34;&gt;Getting the data&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;path_dict = dict(
    confirmed = (&amp;quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/&amp;quot;
                 &amp;quot;csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv&amp;quot;),
    death = (&amp;quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/&amp;quot;
             &amp;quot;csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Deaths.csv&amp;quot;),
    recovered = (&amp;quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/&amp;quot;
                 &amp;quot;csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def set_index(df):
    &amp;quot;&amp;quot;&amp;quot;Set the index for the data frame using the date

    Args:
        df: Pandas data frame obtained from John Hopkins repo
    &amp;quot;&amp;quot;&amp;quot;
    # Set region, country, lat and long as index
    index = pd.MultiIndex.from_frame(df.iloc[:, :4])
    # Set the index and transpose
    df = df.iloc[:, 4:].set_index(index).T
    # Set date as index
    return df.set_index(pd.to_datetime(df.index, dayfirst=False))

df_dict = {
    type_: pd.read_csv(path).pipe(set_index)
    for type_, path in path_dict.items()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;making-interative-plots&#34;&gt;Making interative plots&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def interactive_plot(df_input,
                     df_label,
                     width=800,
                     heigth=600):
    &amp;quot;&amp;quot;&amp;quot;
    Plot the data using ipywidgets to interact with it

    Args:
        df_input: Input data frame, prepared by `set_index`
        df_label: Label of the data frame (death, confirmed, recovered)
        width: Width of the plot
        heigth: Heigth of the plot
    &amp;quot;&amp;quot;&amp;quot;
    # Add a new column with the total number per day
    df_input[(None, &#39;Total&#39;, None, None)] = df_input.groupby(level=1, axis=1).sum().sum(axis=1)

    # Get the list of countries
    country_list = df_input.columns.get_level_values(1).unique().sort_values().tolist()
    # Move Total to the top and add the No country option
    country_list = [&#39;No country&#39;, &#39;Total&#39;] + [c for c in country_list if c != &#39;Total&#39;]

    # Get the list of dates for the series
    date_list = df_input.index.strftime(&#39;%d/%m/%y&#39;)

    # ipywidgets:
    # We use dropdowns for the countries
    country_widget_1 = Dropdown(options=country_list, value=&#39;Total&#39;,
                                description=&#39;Country 1&#39;)
    country_widget_2 = Dropdown(options=country_list, value=&#39;No country&#39;,
                                description=&#39;Country 2&#39;)
    country_widget_3 = Dropdown(options=country_list, value=&#39;No country&#39;,
                                description=&#39;Country 3&#39;)
    # Selection slider for the dates (since the date picker has some problems)
    start_date_widget = SelectionSlider(
        options=date_list,
        value=date_list[0],
        description=&#39;Start date&#39;,
    )
    end_date_widget = SelectionSlider(
        options=date_list,
        value=date_list[-1],
        description=&#39;End date&#39;,
    )
    # And a checkbox to determine if we want the information per day or cumulative
    daily_cases_widget = Checkbox(
        value=False,
        description=&#39;Show the n. of new cases/day&#39;,
    )

    # A layout is used for the plotly figure
    layout = cf.Layout(height=heigth,
                       width=width,
                       autosize=False,
                       xaxis=dict(title=&#39;Date&#39;),
                       margin=dict(l=60, r=60, b=40,
                                   t=40, pad=4))

    # We have a function that does the update of the data
    # using the ipywidgets defined above
    @interact(country_1=country_widget_1,
              country_2=country_widget_2,
              country_3=country_widget_3,
              start_date=start_date_widget,
              end_date=end_date_widget,
              daily_var=daily_cases_widget)
    def plot_range(country_1, country_2, country_3,
                   start_date, end_date, daily_var):
        &amp;quot;&amp;quot;&amp;quot;
        Update the plot using information from the ipywidgets.

        The plot can compare at maximum three countries at the same
        time.

        Args:
            country_1: Country 1 to appear in the plot
            country_2: Country 1 to appear in the plot
            country_3: Country 1 to appear in the plot
            start_date: Plot data from `start_date` on
            end_date: Plot data until `end_date`
            daily_var: Determine if we should show the cumulative
                or the number new cases per day
        &amp;quot;&amp;quot;&amp;quot;
        # Limit the data acconding to the countries selected and date range
        df = (
            df_input
            .groupby(level=1, axis=1).sum()
            .loc[lambda df_: (df_.index &amp;gt;= datetime.strptime(start_date, &#39;%d/%m/%y&#39;)) &amp;amp;
                             (df_.index &amp;lt;= datetime.strptime(end_date, &#39;%d/%m/%y&#39;)),
                 lambda df_: (df_.columns.get_level_values(&#39;Country/Region&#39;)
                              .isin([country_1, country_2, country_3]))]
        )

        # If daily_var is True, compute the number of new cases per day
        if daily_var:
            df = df - df.shift()
            title = F&#39;Number of new {df_label} cases / day&#39;
            plot_type = &#39;bar&#39;
        else:
            title = F&#39;Number of {df_label} cases (cumulative)&#39;
            plot_type = &#39;line&#39;

        # Define the title and y axis label
        layout.title = title
        layout.yaxis.title = title
        # Plot the data
        df.iplot(kind=plot_type, yTitle=title, layout=layout)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;showing-the-data&#34;&gt;Showing the data&lt;/h2&gt;
&lt;p&gt;Finally, we can show our data and play with it.&lt;/p&gt;
&lt;p&gt;Three dropdown menus allow you to select three different countries to compare.&lt;/p&gt;
&lt;p&gt;Run your notebook and see the results :D&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Finally, we plot all the cases
for type_, df in df_dict.items():
    display(Markdown(f&#39;---\n## Number of {type_} cases over time&#39;))
    interactive_plot(df.rename(columns={&#39;Others&#39;: &#39;Diamond Princess&#39;}), type_)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;number-of-confirmed-cases-over-time&#34;&gt;Number of confirmed cases over time&lt;/h2&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;2020-03-13-corona-new-cases.png&#34; &gt;


  &lt;img src=&#34;2020-03-13-corona-new-cases.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;number-of-death-cases-over-time&#34;&gt;Number of death cases over time&lt;/h2&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;2020-03-13-corona-deaths.png&#34; &gt;


  &lt;img src=&#34;2020-03-13-corona-deaths.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;number-of-recovered-cases-over-time&#34;&gt;Number of recovered cases over time&lt;/h2&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;2020-03-13-corona-recovered.png&#34; &gt;


  &lt;img src=&#34;2020-03-13-corona-recovered.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Loading Wikipedia articles (EN) with Python</title>
      <link>https://luizvbo.github.io/post/loading-wikipedia-data-with-python/</link>
      <pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://luizvbo.github.io/post/loading-wikipedia-data-with-python/</guid>
      <description>&lt;p&gt;Wikipedia is the largest and most popular general reference work on the World Wide Web, and is one of the most popular websites ranked by Alexa as of January 2020.&lt;/p&gt;
&lt;p&gt;As of February 2020, there are 6,016,720 articles in the English Wikipedia containing over 3.5 billion words. There are a lot of information about the amount of data in Wikipedia that can be found in 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/1/19/Size_of_English_Wikipedia_graph_2019-03-01.png&#34; alt=&#34;Content page count of the English-language Wikipedia from the beginning to 2019-03-21&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Content page count of the English-language Wikipedia from the beginning to 2019-03-21&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;This article is going to show you how to download the whole Wikipedia (in English) and load the data in Python.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can dowload the original Notebook from my 
&lt;a href=&#34;https://github.com/luizvbo/wikidata/blob/master/wikipedia_store.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;downloading-the-data&#34;&gt;Downloading the Data&lt;/h2&gt;
&lt;p&gt;The fastest way of getting Wikipedia is though torrent. Download the last version of the data dump from this 
&lt;a href=&#34;https://meta.wikimedia.org/wiki/Data_dump_torrents#English_Wikipedia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page&lt;/a&gt;
. You should download the newest version of the file &lt;code&gt;enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2&lt;/code&gt;, where &lt;code&gt;YYYYMMDD&lt;/code&gt; is the date of the dump.&lt;/p&gt;
&lt;p&gt;The MD5 for the file can be found in the link &lt;a href=&#34;http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/md5sums-enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2.txt&#34;&gt;http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/md5sums-enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2.txt&lt;/a&gt; where &lt;code&gt;YYYYMMDD&lt;/code&gt; should be replace by the date of the dump.&lt;/p&gt;
&lt;p&gt;After downloading the articles &lt;strong&gt;bz2&lt;/strong&gt; file, we need to download the list on indices for the articles from &lt;a href=&#34;http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2&#34;&gt;http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2&lt;/a&gt;. Again, replace &lt;code&gt;YYYYMMDD&lt;/code&gt; by the date of the dump.&lt;/p&gt;
&lt;p&gt;After downloading the files you should extract &lt;strong&gt;only the index&lt;/strong&gt; file. On Linux, we can use &lt;code&gt;lbzip2&lt;/code&gt; to uncompress the file using multiple CPUs, speeding up the process. In the terminal, in the file folder type:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lbzip2 -d enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;loading-the-data&#34;&gt;Loading the Data&lt;/h2&gt;
&lt;p&gt;Now is where the things start to become interesting. Since the file is too large to fit in memory, we are going to load it iteratively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pyarrow.parquet as pq
import pyarrow as pa
import pandas as pd
import numpy as np
import itertools
import os
import io

from multiprocessing import Pool
from tqdm import tqdm
from lxml import etree
import bz2
from bz2 import BZ2Decompressor
from typing import (
    List, Generator
)

VERSION = &#39;20200420&#39;

# Path to the bz2 files with Wikipedia data
path_articles = f&#39;enwiki-{VERSION}-pages-articles-multistream.xml.bz2&#39;
# Path to the index list from Wikipedia
path_index = f&#39;enwiki-{VERSION}-pages-articles-multistream-index.txt.bz2&#39;
# Path to our cached version (for offsets)
path_index_clean = f&#39;enwiki-{VERSION}-pages-articles-multistream-index_clean.txt&#39;
# Path to the output parquet file
path_wiki_parquet = &#39;wiki_parquet/&#39;
# Number of processors to be used during processing
n_processors = 16
# Number of blocks of pages to be processed per iteration per processor
n_parallel_blocks = 20
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The multistream dump file contains multiple bz2 &amp;lsquo;streams&amp;rsquo; (bz2 header, body, footer) concatenated together into one file, in contrast to the vanilla file which contains one stream. Each separate &amp;lsquo;stream&amp;rsquo; (or really, file) in the multistream dump contains 100 pages, except possibly the last one. The multistream file allows you to get an article from the archive without unpacking the whole thing.&lt;/p&gt;
&lt;p&gt;The index file contains the full list of articles. The first field of this index is the number of bytes to seek into the compressed archive pages-articles-multistream.xml.bz2, the second is the article ID, the third the article title. A colon (&lt;code&gt;:&lt;/code&gt;) is used to separate fields.&lt;/p&gt;
&lt;p&gt;Since we would like to extract all the articles from wikipedia, we don&amp;rsquo;t have to keep track of titles and IDs, only the offsets. Thus, we read the offsets and store them into a new file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_page_offsets(path_index: str, path_index_clean: str) -&amp;gt; List[int]:
    &amp;quot;&amp;quot;&amp;quot;Get page offsets from wikipedia file or cached version

    Wikipedia provide an index file containing the list of articles with their
    respective id and offset from the start of the file. Since we are
    interested only on the offsets, we read the original file, provided by
    `path_index`, extract the offsets and store in another file (defined by
    `path_index_clean`) to speed up the process

    Args:
        path_index (str): Path to the original index file provided by Wikipedia
            (bz2 compressed version)
        path_index_clean (str): Path to our version, containing only offsets

    Returns:
        List[int]: List of offsets
    &amp;quot;&amp;quot;&amp;quot;
    # Get the list of offsets
    # If our new offset file was not created, it gets the information
    # from the index file
    if not os.path.isfile(path_index_clean):
        # Read the byte offsets from the index file
        page_offset = []
        last_offset = None
        with open(path_index, &#39;rb&#39;) as f:
            b_data = bz2.decompress(f.read()).split(b&#39;\n&#39;)
            # Drop the last line (empty)
            if b_data[-1] == b&#39;&#39;:
                b_data = b_data[:-1]
            for line in tqdm(b_data):
                offset = line.decode().split(&#39;:&#39;, 1)[0]
                if last_offset != offset:
                    last_offset = offset
                    page_offset.append(int(offset))

        with open(path_index_clean, &#39;w&#39;) as f:
            f.write(&#39;,&#39;.join([str(i) for i in page_offset]))
    else:
        with open(path_index_clean, &#39;r&#39;) as f:
            page_offset = [int(idx) for idx in f.read().split(&#39;,&#39;)]

    return page_offset
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;parsing-the-data&#34;&gt;Parsing the data&lt;/h2&gt;
&lt;p&gt;In order to parse the files, we need to open the bz2 file containing the articles, read blocks of bytes, according to the offsets defined above, and then uncompress these blocks.&lt;/p&gt;
&lt;p&gt;The generator &lt;code&gt;get_bz2_byte_str&lt;/code&gt; reads the blocks sequentially, following the list of offsets. And the function &lt;code&gt;get_articles&lt;/code&gt; is used to convert each byte string into a pandas data frame containing the index, title and content of the article.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The XML structure can be found on this 
&lt;a href=&#34;https://meta.wikimedia.org/wiki/Data_dumps/Dump_format&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page&lt;/a&gt;
.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_bz2_byte_str(path_articles: str,
                     offset_list: List[int]) -&amp;gt; Generator[bytes, None, None]:
    &amp;quot;&amp;quot;&amp;quot;Read the multistream bz2 file using the offset list

    The offset list defines where the bz2 (sub)file starts and ends

    Args:
        path_articles (str): Path to the bz2 file containing the Wikipedia
            articles.
        offset_list (List[int]): List of byte offsets

    Yields:
        bytes: String of bytes corresponding to a set of articles compressed
    &amp;quot;&amp;quot;&amp;quot;
    with open(path_articles, &amp;quot;rb&amp;quot;) as f:
        last_offset = offset_list[0]
        # Drop the data before the offset
        f.read(last_offset)
        for next_offset in offset_list[1:]:
            offset = next_offset - last_offset
            last_offset = next_offset
            yield f.read(offset)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_articles(byte_string_compressed: bytes) -&amp;gt; pd.DataFrame:
    &amp;quot;&amp;quot;&amp;quot;Get a dataframe containing the set of articles from a bz2

    Args:
        byte_string_compressed (bytes): Byte string corresponding to the bz2
            stream

    Returns:
        pd.DataFrame: Dataframe with columns title and article
    &amp;quot;&amp;quot;&amp;quot;
    def _get_text(list_xml_el):
        &amp;quot;&amp;quot;&amp;quot;Return the list of content for a list of xml_elements&amp;quot;&amp;quot;&amp;quot;
        return [el.text for el in list_xml_el]

    def _get_id(list_xml_el):
        &amp;quot;&amp;quot;&amp;quot;Return the list of id&#39;s for a list of xml_elements&amp;quot;&amp;quot;&amp;quot;
        return [int(el.text) for el in list_xml_el]

    bz2d = BZ2Decompressor()
    byte_string = bz2d.decompress(byte_string_compressed)
    doc = etree.parse(io.BytesIO(b&#39;&amp;lt;root&amp;gt; &#39; + byte_string + b&#39; &amp;lt;/root&amp;gt;&#39;))

    col_id = _get_id(doc.xpath(&#39;*/id&#39;))
    col_title = _get_text(doc.xpath(&#39;*/title&#39;))
    col_article = _get_text(doc.xpath(&#39;*/revision/text&#39;))

    df = pd.DataFrame([col_id, col_title, col_article],
                      index=[&#39;index&#39;, &#39;title&#39;, &#39;article&#39;]).T
    df[&#39;index&#39;] = df[&#39;index&#39;].astype(np.int32)
    return df
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;reading-and-storing-in-parquet-files&#34;&gt;Reading and storing in parquet files&lt;/h2&gt;
&lt;p&gt;We read the blocks of the bz2 file, extract the data and write to parquet files. In order to speed up the process we use a queue to store the blocks of bytes that are processed in parallel.&lt;/p&gt;
&lt;p&gt;I was having problems to load the index using &lt;strong&gt;dask&lt;/strong&gt;, so I decided to store it as a column and drop the pd.DataFrame index.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def chunks(input_list: List, chunk_size: int) -&amp;gt; Generator[List, None, None]:
    &amp;quot;&amp;quot;&amp;quot;Split a list into chunks of size `chunk_size`

    Args:
        input_list (List): Input list
        chunk_size (int): Size of the chunks. Note that the last chunk may have
            less than `chunk_size` elements

    Yields:
        Generator[List, None, None]: Sublist of size `chunk_size`
    &amp;quot;&amp;quot;&amp;quot;

    # For item i in a range that is a length of l,
    for i in range(0, len(input_list), chunk_size):
        # Create an index range for l of n items:
        yield input_list[i:i+chunk_size]


def _process_parallel(list_bytes: List[bytes]) -&amp;gt; None:
    &amp;quot;&amp;quot;&amp;quot;Process a subset of the byte chunks from the original dump file

    Args:
        list_bytes (List[bytes]): List of byte strings (chunks from the
            original file)
    &amp;quot;&amp;quot;&amp;quot;
    df = pd.concat([get_articles(article) for article in list_bytes])
    output_path = (
        os.path
        .join(path_wiki_parquet,
              &#39;{:08d}.parquet&#39;.format(df[&#39;index&#39;].values[0]))
    )

    # Save the index as a column and ignore the df index
    df.to_parquet(output_path, compression=&#39;snappy&#39;, index=False)

    # Clear the data tables
    del df
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code bellow stores each block of 100 articles in a new parquet file. It allows you to load a subset of the full dump or work using 
&lt;a href=&#34;https://github.com/dask/dask&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dask&lt;/a&gt;
, 
&lt;a href=&#34;https://github.com/modin-project/modin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modin&lt;/a&gt;
 or 
&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pySpark&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;I am using 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Snappy_%28compression%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Snappy&lt;/a&gt;
 to reduce the amount of space used by the extracted data.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Snappy (previously known as Zippy) is a fast data compression and decompression library written in C++ by Google based on ideas from LZ77 and open-sourced in 2011. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;queue = []
page_offset = get_page_offsets(path_index, path_index_clean)
# Read the file sequentially
for bit_str in tqdm(get_bz2_byte_str(path_articles, page_offset), total=len(page_offset)):
    # Feed the queue
    if len(queue) &amp;lt; n_processors * n_parallel_blocks:
        queue.append(bit_str)

    # Decompress and extract the infomation in parallel
    else:
        with Pool(processes=n_processors) as pool:
            tuple(pool.imap_unordered(_process_parallel, chunks(queue, n_parallel_blocks)))
        # Clean the queue
        for el in queue:
            del el
        queue.clear()
# Run one last time
with Pool(processes=n_processors) as pool:
    tuple(pool.imap_unordered(_process_parallel, chunks(queue, n_parallel_blocks)))
# Clean the queue
for el in queue:
    del el
queue.clear()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;Done! Now, we can load the data from parquet.&lt;/p&gt;
&lt;p&gt;However, we still have to parse the data from the articles, since there are some markups used to Wikipedia for citations, info boxes, categories and so on. We will deal with these in the next artile.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
