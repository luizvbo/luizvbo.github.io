[{"authors":["admin"],"categories":null,"content":"I am a Data Scientist working for ING Wholesale Bank Advanced Analytics (WBAA) in Amsterdam.\nI have been working with Machine Learning, Natural Language Processing and Evolutionary Computation in Academia and Industry for the past 10 years with publications in the main Evolutionary Computation conferences (GECCO, CEC, PPSN, EvoStar).\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://luizvbo.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Data Scientist working for ING Wholesale Bank Advanced Analytics (WBAA) in Amsterdam.\nI have been working with Machine Learning, Natural Language Processing and Evolutionary Computation in Academia and Industry for the past 10 years with publications in the main Evolutionary Computation conferences (GECCO, CEC, PPSN, EvoStar).","tags":null,"title":"Luiz Otavio Vilas Boas Oliveira","type":"authors"},{"authors":null,"categories":null,"content":"I have been seen some discussion about epidemiologic models to predict the number of cases and deaths by COVID-19, which made me give some thought about the subject.\nSince we have countries in different stages of the pandemic, my hypothesis was that we could use information from countries in advanced stages to predict information for countries at the beginning of the crisis.\nConsidering the number of deaths by COVID-19 registered, we can align the data for all countries, such that the day the first death by COVID-19 was registered for each country coincides with the origin.\nWith the data shifted, we can compute the correlation between each pair of countries and then find those pairs with high correlation. We can fit a linear regression to each of these pairs considering the country with more data as the independent variable and the one with fewer data as the response variable.\nWe can then use the data for the country with more data to predict the number of deaths for the other country. For, instance, for Brazil, the 8 highest correlated countries are Canada, USA, Egypt, Japan, Argentina, UK, Italy and Iran (all of them with correlation \u0026gt; 0.95).\nI used the data provided by the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) in their GitHub repository .\nThe code for this notebook can be downloaded from here .\nfrom typing import Tuple, Dict import pandas as pd import numpy as np from sklearn.linear_model import LinearRegression from tqdm import tqdm from functools import partial from glob import glob from multiprocessing import Pool import matplotlib.pyplot as plt  # URL to the CSSE repository url_covid_death = (\u0026quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/\u0026quot; \u0026quot;master/csse_covid_19_data/csse_covid_19_time_series/\u0026quot; \u0026quot;time_series_covid19_deaths_global.csv\u0026quot;) # URL to the population data from Worldbank url_pop = (\u0026quot;http://api.worldbank.org/v2/en/indicator/\u0026quot; \u0026quot;SP.POP.TOTL?downloadformat=csv\u0026quot;)  def get_same_origin(df: pd.DataFrame) -\u0026gt; pd.DataFrame: \u0026quot;\u0026quot;\u0026quot;Move first case to the origing Args: df (pd.DataFrame): Input data frame, where each column corresponds to one country. It should be the output of the function `set_index`. Returns: pd.DataFrame: Data frame with every column shifted up \u0026quot;\u0026quot;\u0026quot; n_days = df.shape[0] def _pad_days(s): s = s.astype(float) s_pad = s[s.cumsum() != 0] return np.pad(s_pad, (0, n_days-s_pad.shape[0]), 'constant', constant_values=np.nan) df = ( df.apply(_pad_days, raw=True) .reset_index(drop=True) ).dropna(how='all') return df def set_index(df: pd.DataFrame) -\u0026gt; pd.DataFrame: \u0026quot;\u0026quot;\u0026quot;Set the index for the data frame using the date Args: df (pd.DataFrame): Data frame obtained from John Hopkins repo Returns: pd.DataFrame: Preprocessed data \u0026quot;\u0026quot;\u0026quot; # Set region, country, lat and long as index index = pd.MultiIndex.from_frame(df.iloc[:, :4]) # Set the index and transpose df = df.iloc[:, 4:].set_index(index).T # Set date as index return df.set_index(pd.to_datetime(df.index, dayfirst=False)) def compute_lr(country: Tuple[int, str], df_covid: pd.DataFrame, min_diff: int = 7) -\u0026gt; Dict[Tuple[str, str], Dict]: \u0026quot;\u0026quot;\u0026quot;Fit the logistic regression for each pair of countries Args: country (Tuple[int, str]): Tuple with the index of the country in the column list and the country df_covid (pd.DataFrame): Data frame containing the data shifted to the origin. min_diff (int, optional): Minimum difference in the number of days to consider the country pair. Defaults to 7. Returns: Dict[Tuple[str, str], Dict]: Dictionary where the keys are the pairs of countries and the value is a data dictionary with the data computued. \u0026quot;\u0026quot;\u0026quot; i, col_1 = country results = {} for col_2 in df_covid.columns[i+1:]: x = df_covid[col_1].dropna().values y = df_covid[col_2].dropna().values # Keep the largest array in x if x.shape[0] \u0026lt; y.shape[0]: x, y = y, x x_label, y_label = col_2, col_1 else: x_label, y_label = col_1, col_2 x, y = x.reshape(-1, 1), y.reshape(-1, 1) if x.shape[0] - y.shape[0] \u0026gt; min_diff: lr = LinearRegression() # The weights increase linearly from 1 to 2 weights = np.linspace(1, 1, y.shape[0]) lr.fit(x[:y.shape[0]], y, weights) results[(x_label, y_label)] = dict( lr_model=lr, r_score=lr.score(x[:y.shape[0]], y), predicted=lr.predict(x), x=x, y=y, ) return results def plot_candidates(df_candidates: pd.DataFrame, nrows: int = 4, ncols: int = 2, over_days: bool = True, figsize: Tuple[int, int] = (12, 15)): \u0026quot;\u0026quot;\u0026quot;Plot the regression for pairs of countries Args: df_candidates (pd.DataFrame): Data frame with pairs of countries with the linear regression data nrows (int, optional): Number of rows to plot. Defaults to 4. ncols (int, optional): NBumber of columns to plot. Defaults to 2. over_days (bool, optional): If True, plot the data over days. Otherwise, plot one country agains the other. Defaults to True. figsize (Tuple[int, int], optional): Size of the resulting fiture. Defaults to (12, 15). Returns: [type]: Figure object \u0026quot;\u0026quot;\u0026quot; fig, axs = plt.subplots(nrows, ncols) df_ = df_candidates.head(nrows * ncols) for (i, row), ax in zip(df_.iterrows(), axs.flatten()): if over_days: ax.plot(row['x'], label=i[0]) ax.plot(row['y'], label=i[1]) ax.plot(row['predicted'], '--', label=f\u0026quot;{i[1]} (predicted)\u0026quot;) ax.set_xlabel(\u0026quot;Days since the first death by COVID-19\u0026quot;) ax.set_ylabel(\u0026quot;Number of deaths\u0026quot;) else: ax.plot(row['x'][:row['y'].shape[0]], row['y'], label='True value') ax.plot(row['x'], row['predicted'], '--', label='Predicted') ax.set_xlabel(i[0]) ax.set_ylabel(i[1]) ax.grid(True) ax.legend(title=\u0026quot;$r^2={:.3f}$\u0026quot;.format(row['r_score']), loc='upper left') fig.set_size_inches(*figsize) return fig  # Getting the population information !wget {url_pop} -O pop.zip !unzip -o pop.zip -d pop_csv # Load the population file df_pop_ = pd.read_csv(glob('pop_csv/API_SP.POP.TOTL*.csv')[0], skiprows=4) df_pop = df_pop_[['Country Name', '2018']].set_index('Country Name') # Delete the files downloaded !rm -r pop_csv  --2020-05-11 22:03:17-- http://api.worldbank.org/v2/en/indicator/SP.POP.TOTL?downloadformat=csv Resolving api.worldbank.org (api.worldbank.org)... 34.237.118.134 Connecting to api.worldbank.org (api.worldbank.org)|34.237.118.134|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 78803 (77K) [application/zip] Saving to: ‘pop.zip’ pop.zip 100%[===================\u0026gt;] 76.96K 152KB/s in 0.5s 2020-05-11 22:03:19 (152 KB/s) - ‘pop.zip’ saved [78803/78803] Archive: pop.zip inflating: pop_csv/Metadata_Indicator_API_SP.POP.TOTL_DS2_en_csv_v2_988606.csv inflating: pop_csv/API_SP.POP.TOTL_DS2_en_csv_v2_988606.csv inflating: pop_csv/Metadata_Country_API_SP.POP.TOTL_DS2_en_csv_v2_988606.csv  # Loading the data for the number of Deaths df_death = pd.read_csv(url_covid_death) df = set_index(df_death) # Groupy territories per country df = df.groupby(level=1, axis=1).sum() # # Drop all-zeros columns df = df[df.sum()[lambda s: s \u0026gt; 0].index] # # Shift all series to the origin (first death) df = get_same_origin(df)  # Ignore countries with less than 1M countries = [c for c in df.columns if c not in df_pop[lambda df_: df_['2018'] \u0026lt; 10**6].index] compute_lr_parallel = partial(compute_lr, df_covid=df[countries]) with Pool(8) as pool: results = {} for res_dict in tqdm(pool.imap(compute_lr_parallel, enumerate(countries)), total=df.shape[0]): results.update(res_dict) df_results = pd.DataFrame.from_dict(results, orient='index')  144it [00:02, 50.77it/s]  Plots Linear Regression Results The first set of figures shows the output for the linear regression, considering the number of deaths from Brazil as the response variable (y-axis) and each of the other countries as the dependent variable (x-axis).\ndf_brazil = df_results[lambda df: (df.index.get_level_values(1) == 'Brazil')].sort_values('r_score', ascending=False) fig1 = plot_candidates(df_brazil, over_days=False)  Predictions The second set shows the prediction, for the number of deaths in Brazil, according to the line fitted to each of the other countries. The number of days in future is limited by the data available for the other country.\nfig2 = plot_candidates(df_, over_days=True)  ","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"730f404fa30353d0badb6e237d072fc0","permalink":"https://luizvbo.github.io/post/corona-linear-regression/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/post/corona-linear-regression/","section":"post","summary":"I have been seen some discussion about epidemiologic models to predict the number of cases and deaths by COVID-19, which made me give some thought about the subject.\nSince we have countries in different stages of the pandemic, my hypothesis was that we could use information from countries in advanced stages to predict information for countries at the beginning of the crisis.\nConsidering the number of deaths by COVID-19 registered, we can align the data for all countries, such that the day the first death by COVID-19 was registered for each country coincides with the origin.","tags":null,"title":"Predicting COVID-19 Deaths by Similarity","type":"post"},{"authors":null,"categories":null,"content":"I would like to see: The evolution over time of the number of cases/deaths/recovery patients per country. Assuming that others are also interested in this information, I am sharing the plots I made with plotly.\nThe Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a GitHub repository with daily updated information about the Corona Virus . They also provide a dashboard to interact with the data (however, even their dashboard doesn’t provide the plot I was looking for).\nYou are more than welcome to modify the notebook to predict the number of cases and do other types of analysis. The notebook can be found in my  GitHub repository .\nThe data is split into three plots:\n  COVID-19 confirmed cases   COVID-19 deaths   COVID-19 recovered cases   Bellow, you can see the data for all the confirmed cases (total and per day).\nYou can select countries to compare the data. If you are having problems to visualise the plot, you can try the clean html version from this link ' Data Shifted to Origin I have been seeing plots where they compare the number of new cases/deaths among different countries by moving the origin of the plots the day of the first case/death. Thus, I did the same for the plot above: all the data points are shifted in the horizontal axis such that the origin corresponds to first case/death for that country:\nNOTE: The data IS NOT normalised by the population size.\n'","date":1584748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584748800,"objectID":"62c9504140bd79f9ca1c6b23c335e17f","permalink":"https://luizvbo.github.io/post/coronavirus-confirmed/","publishdate":"2020-03-21T00:00:00Z","relpermalink":"/post/coronavirus-confirmed/","section":"post","summary":"I would like to see: The evolution over time of the number of cases/deaths/recovery patients per country. Assuming that others are also interested in this information, I am sharing the plots I made with plotly.\nThe Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a GitHub repository with daily updated information about the Corona Virus . They also provide a dashboard to interact with the data (however, even their dashboard doesn’t provide the plot I was looking for).","tags":null,"title":"Visualizing the number of COVID-19 confirmed cases with Plotly","type":"post"},{"authors":null,"categories":null,"content":"I would like to see: The evolution over time of the number of cases/deaths/recovery patients per country. Assuming that others are also interested in this information, I am sharing the plots I made with plotly.\nThe Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a GitHub repository with daily updated information about the Corona Virus . They also provide a dashboard to interact with the data (however, even their dashboard doesn’t provide the plot I was looking for).\nYou are more than welcome to modify the notebook to predict the number of cases and do other types of analysis. The notebook can be found in my  GitHub repository .\nThe data is split into three plots:\n  COVID-19 confirmed cases   COVID-19 deaths   COVID-19 recovered cases   Bellow, you can see the data for the deaths caused by COVID-19 (total and per day).\nYou can select countries to compare the data. If you are having problems to visualise the plot, you can try the clean html version from this link ' Data Shifted to Origin I have been seeing plots where they compare the number of new cases/deaths among different countries by moving the origin of the plots the day of the first case/death. Thus, I did the same for the plot above: all the data points are shifted in the horizontal axis such that the origin corresponds to first case/death for that country:\nNOTE: The data IS NOT normalised by the population size.\n'","date":1584748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584748800,"objectID":"ec07838aa30dfc6d702e9eadcf3a6d50","permalink":"https://luizvbo.github.io/post/coronavirus-death/","publishdate":"2020-03-21T00:00:00Z","relpermalink":"/post/coronavirus-death/","section":"post","summary":"I would like to see: The evolution over time of the number of cases/deaths/recovery patients per country. Assuming that others are also interested in this information, I am sharing the plots I made with plotly.\nThe Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a GitHub repository with daily updated information about the Corona Virus . They also provide a dashboard to interact with the data (however, even their dashboard doesn’t provide the plot I was looking for).","tags":null,"title":"Visualizing the number of COVID-19 deaths with Plotly","type":"post"},{"authors":null,"categories":null,"content":"I would like to see: The evolution over time of the number of cases/deaths/recovery patients per country. Assuming that others are also interested in this information, I am sharing the plots I made with plotly.\nThe Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a GitHub repository with daily updated information about the Corona Virus . They also provide a dashboard to interact with the data (however, even their dashboard doesn’t provide the plot I was looking for).\nYou are more than welcome to modify the notebook to predict the number of cases and do other types of analysis. The notebook can be found in my  GitHub repository .\nThe data is split into three plots:\n  COVID-19 confirmed cases   COVID-19 deaths   COVID-19 recovered cases   Bellow, you can see the data for all the recovered cases (total and per day).\nNOTE: Starting from 25/03/2020, the JHU CSSE is not updating the number of recovered cases anymore. Thus, the plot bellow will not be updated anymore.\nYou can select countries to compare the data. If you are having problems to visualise the plot, you can try the clean html version from this link ' ","date":1584748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584748800,"objectID":"238bc555c50bd92e49416079e6f35286","permalink":"https://luizvbo.github.io/post/coronavirus-recovered/","publishdate":"2020-03-21T00:00:00Z","relpermalink":"/post/coronavirus-recovered/","section":"post","summary":"I would like to see: The evolution over time of the number of cases/deaths/recovery patients per country. Assuming that others are also interested in this information, I am sharing the plots I made with plotly.\nThe Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a GitHub repository with daily updated information about the Corona Virus . They also provide a dashboard to interact with the data (however, even their dashboard doesn’t provide the plot I was looking for).","tags":null,"title":"Visualizing the number of COVID-19 recovered cases with Plotly","type":"post"},{"authors":null,"categories":null,"content":"Exploring COVID-19 The Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a GitHub repository with daily updated information about the Corona Virus . They also provide a dashboard to interact with the data.\nHowever, even their dashboard doesn’t provide the information I would like to see: The evolution over time of the number of cases/deaths/recovery patients per country.\nThus, the idea of this notebook is to create interactive plots that we can use to monitor the evolution of the disease.\nYou are more than welcome to modify the notebook to predict the number of cases and do other types of analysis. The notebook can be found in my  GitHub repository .\nImports I am using Pandas + cufflinks to plot our data using Plotly.\nI am also using ipywidgets to interact with the plots. If you are using Jupyterlab, you may need to refer to the installation documention .\nimport pandas as pd from IPython.display import Markdown, display from ipywidgets import interact from ipywidgets.widgets import ( Dropdown, SelectionSlider, Checkbox ) from datetime import datetime import cufflinks as cf  window.PlotlyConfig = {MathJaxConfig: 'local'}; if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} if (typeof require !== 'undefined') { require.undef(\"plotly\"); requirejs.config({ paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min'] } }); require(['plotly'], function(Plotly) { window._Plotly = Plotly; }); }  Getting the data path_dict = dict( confirmed = (\u0026quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/\u0026quot; \u0026quot;csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv\u0026quot;), death = (\u0026quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/\u0026quot; \u0026quot;csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Deaths.csv\u0026quot;), recovered = (\u0026quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/\u0026quot; \u0026quot;csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv\u0026quot;) )  def set_index(df): \u0026quot;\u0026quot;\u0026quot;Set the index for the data frame using the date Args: df: Pandas data frame obtained from John Hopkins repo \u0026quot;\u0026quot;\u0026quot; # Set region, country, lat and long as index index = pd.MultiIndex.from_frame(df.iloc[:, :4]) # Set the index and transpose df = df.iloc[:, 4:].set_index(index).T # Set date as index return df.set_index(pd.to_datetime(df.index, dayfirst=False)) df_dict = { type_: pd.read_csv(path).pipe(set_index) for type_, path in path_dict.items() }  Making interative plots def interactive_plot(df_input, df_label, width=800, heigth=600): \u0026quot;\u0026quot;\u0026quot; Plot the data using ipywidgets to interact with it Args: df_input: Input data frame, prepared by `set_index` df_label: Label of the data frame (death, confirmed, recovered) width: Width of the plot heigth: Heigth of the plot \u0026quot;\u0026quot;\u0026quot; # Add a new column with the total number per day df_input[(None, 'Total', None, None)] = df_input.groupby(level=1, axis=1).sum().sum(axis=1) # Get the list of countries country_list = df_input.columns.get_level_values(1).unique().sort_values().tolist() # Move Total to the top and add the No country option country_list = ['No country', 'Total'] + [c for c in country_list if c != 'Total'] # Get the list of dates for the series date_list = df_input.index.strftime('%d/%m/%y') # ipywidgets: # We use dropdowns for the countries country_widget_1 = Dropdown(options=country_list, value='Total', description='Country 1') country_widget_2 = Dropdown(options=country_list, value='No country', description='Country 2') country_widget_3 = Dropdown(options=country_list, value='No country', description='Country 3') # Selection slider for the dates (since the date picker has some problems) start_date_widget = SelectionSlider( options=date_list, value=date_list[0], description='Start date', ) end_date_widget = SelectionSlider( options=date_list, value=date_list[-1], description='End date', ) # And a checkbox to determine if we want the information per day or cumulative daily_cases_widget = Checkbox( value=False, description='Show the n. of new cases/day', ) # A layout is used for the plotly figure layout = cf.Layout(height=heigth, width=width, autosize=False, xaxis=dict(title='Date'), margin=dict(l=60, r=60, b=40, t=40, pad=4)) # We have a function that does the update of the data # using the ipywidgets defined above @interact(country_1=country_widget_1, country_2=country_widget_2, country_3=country_widget_3, start_date=start_date_widget, end_date=end_date_widget, daily_var=daily_cases_widget) def plot_range(country_1, country_2, country_3, start_date, end_date, daily_var): \u0026quot;\u0026quot;\u0026quot; Update the plot using information from the ipywidgets. The plot can compare at maximum three countries at the same time. Args: country_1: Country 1 to appear in the plot country_2: Country 1 to appear in the plot country_3: Country 1 to appear in the plot start_date: Plot data from `start_date` on end_date: Plot data until `end_date` daily_var: Determine if we should show the cumulative or the number new cases per day \u0026quot;\u0026quot;\u0026quot; # Limit the data acconding to the countries selected and date range df = ( df_input .groupby(level=1, axis=1).sum() .loc[lambda df_: (df_.index \u0026gt;= datetime.strptime(start_date, '%d/%m/%y')) \u0026amp; (df_.index \u0026lt;= datetime.strptime(end_date, '%d/%m/%y')), lambda df_: (df_.columns.get_level_values('Country/Region') .isin([country_1, country_2, country_3]))] ) # If daily_var is True, compute the number of new cases per day if daily_var: df = df - df.shift() title = F'Number of new {df_label} cases / day' plot_type = 'bar' else: title = F'Number of {df_label} cases (cumulative)' plot_type = 'line' # Define the title and y axis label layout.title = title layout.yaxis.title = title # Plot the data df.iplot(kind=plot_type, yTitle=title, layout=layout)  Showing the data Finally, we can show our data and play with it.\nThree dropdown menus allow you to select three different countries to compare.\nRun your notebook and see the results :D\n# Finally, we plot all the cases for type_, df in df_dict.items(): display(Markdown(f'---\\n## Number of {type_} cases over time')) interactive_plot(df.rename(columns={'Others': 'Diamond Princess'}), type_)   Number of confirmed cases over time     Number of death cases over time     Number of recovered cases over time    ","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583971200,"objectID":"ef18e4df085d453b97f4450123c447f1","permalink":"https://luizvbo.github.io/post/coronavirus-visualization/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/post/coronavirus-visualization/","section":"post","summary":"Exploring COVID-19 The Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) maintain a GitHub repository with daily updated information about the Corona Virus . They also provide a dashboard to interact with the data.\nHowever, even their dashboard doesn’t provide the information I would like to see: The evolution over time of the number of cases/deaths/recovery patients per country.\nThus, the idea of this notebook is to create interactive plots that we can use to monitor the evolution of the disease.","tags":null,"title":"Visualizing COVID-19 infection (corona virus) over time","type":"post"},{"authors":null,"categories":null,"content":"Wikipedia is the largest and most popular general reference work on the World Wide Web, and is one of the most popular websites ranked by Alexa as of January 2020.\nAs of February 2020, there are 6,016,720 articles in the English Wikipedia containing over 3.5 billion words. There are a lot of information about the amount of data in Wikipedia that can be found in this article .\n Content page count of the English-language Wikipedia from the beginning to 2019-03-21\n This article is going to show you how to download the whole Wikipedia (in English) and load the data in Python.\nYou can dowload the original Notebook from my GitHub repository .\nDownloading the Data The fastest way of getting Wikipedia is though torrent. Download the last version of the data dump from this page . You should download the newest version of the file enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2, where YYYYMMDD is the date of the dump.\nThe MD5 for the file can be found in the link http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/md5sums-enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2.txt where YYYYMMDD should be replace by the date of the dump.\nAfter downloading the articles bz2 file, we need to download the list on indices for the articles from http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2. Again, replace YYYYMMDD by the date of the dump.\nAfter downloading the files you should extract only the index file. On Linux, we can use lbzip2 to uncompress the file using multiple CPUs, speeding up the process. In the terminal, in the file folder type:\n$ lbzip2 -d enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2  Loading the Data Now is where the things start to become interesting. Since the file is too large to fit in memory, we are going to load it iteratively.\nimport pyarrow.parquet as pq import pyarrow as pa import pandas as pd import numpy as np import itertools import os import io from multiprocessing import Pool from tqdm import tqdm from lxml import etree import bz2 from bz2 import BZ2Decompressor from typing import ( List, Generator ) VERSION = '20200420' # Path to the bz2 files with Wikipedia data path_articles = f'enwiki-{VERSION}-pages-articles-multistream.xml.bz2' # Path to the index list from Wikipedia path_index = f'enwiki-{VERSION}-pages-articles-multistream-index.txt.bz2' # Path to our cached version (for offsets) path_index_clean = f'enwiki-{VERSION}-pages-articles-multistream-index_clean.txt' # Path to the output parquet file path_wiki_parquet = 'wiki_parquet/' # Number of processors to be used during processing n_processors = 16 # Number of blocks of pages to be processed per iteration per processor n_parallel_blocks = 20  The multistream dump file contains multiple bz2 \u0026lsquo;streams\u0026rsquo; (bz2 header, body, footer) concatenated together into one file, in contrast to the vanilla file which contains one stream. Each separate \u0026lsquo;stream\u0026rsquo; (or really, file) in the multistream dump contains 100 pages, except possibly the last one. The multistream file allows you to get an article from the archive without unpacking the whole thing.\nThe index file contains the full list of articles. The first field of this index is the number of bytes to seek into the compressed archive pages-articles-multistream.xml.bz2, the second is the article ID, the third the article title. A colon (:) is used to separate fields.\nSince we would like to extract all the articles from wikipedia, we don\u0026rsquo;t have to keep track of titles and IDs, only the offsets. Thus, we read the offsets and store them into a new file.\ndef get_page_offsets(path_index: str, path_index_clean: str) -\u0026gt; List[int]: \u0026quot;\u0026quot;\u0026quot;Get page offsets from wikipedia file or cached version Wikipedia provide an index file containing the list of articles with their respective id and offset from the start of the file. Since we are interested only on the offsets, we read the original file, provided by `path_index`, extract the offsets and store in another file (defined by `path_index_clean`) to speed up the process Args: path_index (str): Path to the original index file provided by Wikipedia (bz2 compressed version) path_index_clean (str): Path to our version, containing only offsets Returns: List[int]: List of offsets \u0026quot;\u0026quot;\u0026quot; # Get the list of offsets # If our new offset file was not created, it gets the information # from the index file if not os.path.isfile(path_index_clean): # Read the byte offsets from the index file page_offset = [] last_offset = None with open(path_index, 'rb') as f: b_data = bz2.decompress(f.read()).split(b'\\n') # Drop the last line (empty) if b_data[-1] == b'': b_data = b_data[:-1] for line in tqdm(b_data): offset = line.decode().split(':', 1)[0] if last_offset != offset: last_offset = offset page_offset.append(int(offset)) with open(path_index_clean, 'w') as f: f.write(','.join([str(i) for i in page_offset])) else: with open(path_index_clean, 'r') as f: page_offset = [int(idx) for idx in f.read().split(',')] return page_offset  Parsing the data In order to parse the files, we need to open the bz2 file containing the articles, read blocks of bytes, according to the offsets defined above, and then uncompress these blocks.\nThe generator get_bz2_byte_str reads the blocks sequentially, following the list of offsets. And the function get_articles is used to convert each byte string into a pandas data frame containing the index, title and content of the article.\nNote: The XML structure can be found on this page .\ndef get_bz2_byte_str(path_articles: str, offset_list: List[int]) -\u0026gt; Generator[bytes, None, None]: \u0026quot;\u0026quot;\u0026quot;Read the multistream bz2 file using the offset list The offset list defines where the bz2 (sub)file starts and ends Args: path_articles (str): Path to the bz2 file containing the Wikipedia articles. offset_list (List[int]): List of byte offsets Yields: bytes: String of bytes corresponding to a set of articles compressed \u0026quot;\u0026quot;\u0026quot; with open(path_articles, \u0026quot;rb\u0026quot;) as f: last_offset = offset_list[0] # Drop the data before the offset f.read(last_offset) for next_offset in offset_list[1:]: offset = next_offset - last_offset last_offset = next_offset yield f.read(offset)  def get_articles(byte_string_compressed: bytes) -\u0026gt; pd.DataFrame: \u0026quot;\u0026quot;\u0026quot;Get a dataframe containing the set of articles from a bz2 Args: byte_string_compressed (bytes): Byte string corresponding to the bz2 stream Returns: pd.DataFrame: Dataframe with columns title and article \u0026quot;\u0026quot;\u0026quot; def _get_text(list_xml_el): \u0026quot;\u0026quot;\u0026quot;Return the list of content for a list of xml_elements\u0026quot;\u0026quot;\u0026quot; return [el.text for el in list_xml_el] def _get_id(list_xml_el): \u0026quot;\u0026quot;\u0026quot;Return the list of id's for a list of xml_elements\u0026quot;\u0026quot;\u0026quot; return [int(el.text) for el in list_xml_el] bz2d = BZ2Decompressor() byte_string = bz2d.decompress(byte_string_compressed) doc = etree.parse(io.BytesIO(b'\u0026lt;root\u0026gt; ' + byte_string + b' \u0026lt;/root\u0026gt;')) col_id = _get_id(doc.xpath('*/id')) col_title = _get_text(doc.xpath('*/title')) col_article = _get_text(doc.xpath('*/revision/text')) df = pd.DataFrame([col_id, col_title, col_article], index=['index', 'title', 'article']).T df['index'] = df['index'].astype(np.int32) return df  Reading and storing in parquet files We read the blocks of the bz2 file, extract the data and write to parquet files. In order to speed up the process we use a queue to store the blocks of bytes that are processed in parallel.\nI was having problems to load the index using dask, so I decided to store it as a column and drop the pd.DataFrame index.\ndef chunks(input_list: List, chunk_size: int) -\u0026gt; Generator[List, None, None]: \u0026quot;\u0026quot;\u0026quot;Split a list into chunks of size `chunk_size` Args: input_list (List): Input list chunk_size (int): Size of the chunks. Note that the last chunk may have less than `chunk_size` elements Yields: Generator[List, None, None]: Sublist of size `chunk_size` \u0026quot;\u0026quot;\u0026quot; # For item i in a range that is a length of l, for i in range(0, len(input_list), chunk_size): # Create an index range for l of n items: yield input_list[i:i+chunk_size] def _process_parallel(list_bytes: List[bytes]) -\u0026gt; None: \u0026quot;\u0026quot;\u0026quot;Process a subset of the byte chunks from the original dump file Args: list_bytes (List[bytes]): List of byte strings (chunks from the original file) \u0026quot;\u0026quot;\u0026quot; df = pd.concat([get_articles(article) for article in list_bytes]) output_path = ( os.path .join(path_wiki_parquet, '{:08d}.parquet'.format(df['index'].values[0])) ) # Save the index as a column and ignore the df index df.to_parquet(output_path, compression='snappy', index=False) # Clear the data tables del df  The code bellow stores each block of 100 articles in a new parquet file. It allows you to load a subset of the full dump or work using Dask , Modin or pySpark .\nI am using Snappy to reduce the amount of space used by the extracted data.\n Snappy (previously known as Zippy) is a fast data compression and decompression library written in C++ by Google based on ideas from LZ77 and open-sourced in 2011. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression.\n queue = [] page_offset = get_page_offsets(path_index, path_index_clean) # Read the file sequentially for bit_str in tqdm(get_bz2_byte_str(path_articles, page_offset), total=len(page_offset)): # Feed the queue if len(queue) \u0026lt; n_processors * n_parallel_blocks: queue.append(bit_str) # Decompress and extract the infomation in parallel else: with Pool(processes=n_processors) as pool: tuple(pool.imap_unordered(_process_parallel, chunks(queue, n_parallel_blocks))) # Clean the queue for el in queue: del el queue.clear() # Run one last time with Pool(processes=n_processors) as pool: tuple(pool.imap_unordered(_process_parallel, chunks(queue, n_parallel_blocks))) # Clean the queue for el in queue: del el queue.clear()  Next Steps Done! Now, we can load the data from parquet.\nHowever, we still have to parse the data from the articles, since there are some markups used to Wikipedia for citations, info boxes, categories and so on. We will deal with these in the next artile.\n","date":1583280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583280000,"objectID":"5950d3db8a1893612f2833afb098124d","permalink":"https://luizvbo.github.io/post/loading-wikipedia-data-with-python/","publishdate":"2020-03-04T00:00:00Z","relpermalink":"/post/loading-wikipedia-data-with-python/","section":"post","summary":"Wikipedia is the largest and most popular general reference work on the World Wide Web, and is one of the most popular websites ranked by Alexa as of January 2020.\nAs of February 2020, there are 6,016,720 articles in the English Wikipedia containing over 3.5 billion words. There are a lot of information about the amount of data in Wikipedia that can be found in this article .\n Content page count of the English-language Wikipedia from the beginning to 2019-03-21","tags":null,"title":"Loading Wikipedia articles (EN) with Python","type":"post"},{"authors":["Luiz Otavio V. B. Oliveira","Joao Francisco B. S. Martins","Luis F. Miranda","Gisele L. Pappa"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"63083184dd000c204a103d9f25d0b70b","permalink":"https://luizvbo.github.io/publication/oliveira-2018-analysing/","publishdate":"2020-02-09T16:34:20.255403Z","relpermalink":"/publication/oliveira-2018-analysing/","section":"publication","summary":"The definition of a concise and effective testbed for Genetic Programming (GP) is a recurrent matter in the research community. This paper takes a new step in this direction, proposing a different approach to measure the quality of the symbolic regression benchmarks quantitatively. The proposed approach is based on meta-learning and uses a set of dataset meta-features---such as the number of examples or output skewness---to describe the datasets. Our idea is to correlate these meta-features with the errors obtained by a GP method. These meta-features define a space of benchmarks that should, ideally, have datasets (points) covering different regions of the space. An initial analysis of 63 datasets showed that current benchmarks are concentrated in a small region of this benchmark space. We also found out that number of instances and output skewness are the most relevant meta-features to GP output error. Both conclusions can help define which datasets should compose an effective testbed for symbolic regression methods.","tags":null,"title":"Analysing Symbolic Regression Benchmarks under a Meta-Learning Approach","type":"publication"},{"authors":["Joao Francisco B. S. Martins","Luiz Otavio V. B. Oliveira","Luis F. Miranda","Felipe Casadei","Gisele L. Pappa"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"be8964e0fa44693570765b21e48ed391","permalink":"https://luizvbo.github.io/publication/martins-2018-solving/","publishdate":"2020-02-09T16:34:20.273694Z","relpermalink":"/publication/martins-2018-solving/","section":"publication","summary":"Advances in Geometric Semantic Genetic Programming (GSGP) have shown that this variant of Genetic Programming (GP) reaches better results than its predecessor for supervised machine learning problems, particularly in the task of symbolic regression. However, by construction, the geometric semantic crossover operator generates individuals that grow exponentially with the number of generations, resulting in solutions with limited use. This paper presents a new method for individual simplification named GSGP with Reduced trees (GSGP-Red). GSGP-Red works by expanding the functions generated by the geometric semantic operators. The resulting expanded function is guaranteed to be a linear combination that, in a second step, has its repeated structures and respective coefficients aggregated. Experiments in 12 real-world datasets show that it is not only possible to create smaller and completely equivalent individuals in competitive computational time, but also to reduce the number of nodes composing them by 58 orders of magnitude, on average.","tags":null,"title":"Solving the Exponential Growth of Symbolic Regression Trees in Geometric Semantic Genetic Programming","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Fernando E. B. Otero","Gisele L. Pappa"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"6b2fb56c37e43a9fdf2a760a4c0d10fd","permalink":"https://luizvbo.github.io/publication/oliveira-2017-generic/","publishdate":"2020-02-09T16:34:20.254662Z","relpermalink":"/publication/oliveira-2017-generic/","section":"publication","summary":"This chapter proposes a generic framework to build geometric dispersion (GD) operators for Geometric Semantic Genetic Programming in the context of symbolic regression, followed by two concrete instantiations of the framework: a multiplicative geometric dispersion operator and an additive geometric dispersion operator. These operators move individuals in the semantic space in order to balance the population around the target output in each dimension, with the objective of expanding the convex hull defined by the population to include the desired output vector. An experimental analysis was conducted in a testbed composed of sixteen datasets showing that dispersion operators can improve GSGP search and that the multiplicative version of the operator is overall better than the additive version.","tags":null,"title":"A Generic Framework for Building Dispersion Operators in the Semantic Space","type":"publication"},{"authors":["Luis F. Miranda","Luiz Otavio V. B. Oliveira","Joao F. B. S. Martins","Gisele L. Pappa"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c5ef87de63a5fd201fa9bee1ef964b53","permalink":"https://luizvbo.github.io/publication/miranda-2017-hownoisy/","publishdate":"2020-02-09T16:34:20.269607Z","relpermalink":"/publication/miranda-2017-hownoisy/","section":"publication","summary":"Noise is a consequence of acquiring and pre-processing data from the environment, and shows fluctuations from different sources---e.g., from sensors, signal processing technology or even human error. As a machine learning technique, Genetic Programming (GP) is not immune to this problem, which the field has frequently addressed. Recently, Geometric Semantic Genetic Programming (GSGP), a semantic-aware branch of GP, has shown robustness and high generalization capability. Researchers believe these characteristics may be associated with a lower sensibility to noisy data. However, there is no systematic study on this matter. This paper performs a deep analysis of the GSGP performance over the presence of noise. Using 15 synthetic datasets where noise can be controlled, we added different ratios of noise to the data and compared the results obtained with those of a canonical GP. The results show that, as we increase the percentage of noisy instances, the generalization performance degradation is more pronounced in GSGP than GP. However, in general, GSGP is more robust to noise than GP in the presence of up to 10% of noise, and presents no statistical difference for values higher than that in the test bed.","tags":null,"title":"How Noisy Data Affects Geometric Semantic Genetic Programming","type":"publication"},{"authors":["Alex G. C. de Sá","Walter José G. S. Pinto","Luiz Otavio V. B. Oliveira","Gisele L. Pappa"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"a046df81b9c55e8b6aeabee9df636319","permalink":"https://luizvbo.github.io/publication/desa-2017-recipe/","publishdate":"2020-02-09T16:34:20.271826Z","relpermalink":"/publication/desa-2017-recipe/","section":"publication","summary":"Automatic Machine Learning is a growing area of machine learning that has a similar objective to the area of hyper-heuristics: to automatically recommend optimized pipelines, algorithms or appropriate parameters to specific tasks without much dependency on user knowledge. The background knowledge required to solve the task at hand is actually embedded into a search mechanism that builds personalized solutions to the task. Following this idea, this paper proposes RECIPE (REsilient ClassifIcation Pipeline Evolution), a framework based on grammar-based genetic programming that builds customized classification pipelines. The framework is flexible enough to receive different grammars and can be easily extended to other machine learning tasks. RECIPE overcomes the drawbacks of previous evolutionary-based frameworks, such as generating invalid individuals, and organizes a high number of possible suitable data pre-processing and classification methods into a grammar. Results of f-measure obtained by RECIPE are compared to those two state-of-the-art methods, and shown to be as good as or better than those previously reported in the literature. RECIPE represents a first step towards a complete framework for dealing with different machine learning tasks with the minimum required human intervention.","tags":null,"title":"RECIPE: A Grammar-Based Framework for Automatically Evolving Classification Pipelines","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Felipe Casadei","Gisele L. Pappa"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"24ba42ae872e812ee4bf3bc2af2974e0","permalink":"https://luizvbo.github.io/publication/oliveira-2017-strategies/","publishdate":"2020-02-09T16:34:20.274173Z","relpermalink":"/publication/oliveira-2017-strategies/","section":"publication","summary":"In the last years, different approaches have been proposed to introduce semantic information to genetic programming. In particular, the geometric semantic genetic programming (GSGP) and the interesting properties of its evolutionary operators have gotten the attention of the community. This paper is interested in the use of GSGP to solve symbolic regression problems, where semantics is defined by the output set generated by a given individual when applied to the training cases. In this scenario, both mutation and crossover operators defined with fitness function based on Manhattan distance use randomly built functions to generate offspring. However, the outputs of these random functions are not guaranteed to be uniformly distributed in the semantic space, as the functions are generated considering the syntactic space. We hypothesize that the non-uniformity of the semantics of these functions may bias the search, and propose three different standard normalization techniques to improve the distribution of the outputs of these random functions over the semantic space. The results are compared with a popular strategy that uses a logistic function as a wrapper to the outputs, and show that the strategies tested can improve the results of the previous method. The experimental analysis also indicates that a more uniform distribution of the semantics of these functions does not necessarily imply in better results in terms of test error.","tags":null,"title":"Strategies for Improving the Distribution of Random Function Outputs in GSGP","type":"publication"},{"authors":["L. O. V. B. Oliveira"],"categories":null,"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"d6a319fcef9a0112728114a048ee97ce","permalink":"https://luizvbo.github.io/publication/oliveira-2016-improving/","publishdate":"2020-02-09T16:34:20.270186Z","relpermalink":"/publication/oliveira-2016-improving/","section":"publication","summary":"","tags":null,"title":"Improving Search in Geometric Semantic Genetic Programming","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Fernando E. B. Otero","Gisele L. Pappa"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"d327a5c12264f43251212015b8e3a849","permalink":"https://luizvbo.github.io/publication/oliveira-2016-dispersion/","publishdate":"2020-02-09T16:34:20.254282Z","relpermalink":"/publication/oliveira-2016-dispersion/","section":"publication","summary":"Recent advances in geometric semantic genetic programming (GSGP) have shown that the results obtained by these methods can outperform those obtained by classical genetic programming algorithms, in particular in the context of symbolic regression. However, there are still many open issues on how to improve their search mechanism. One of these issues is how to get around the fact that the GSGP crossover operator cannot generate solutions that are placed outside the convex hull formed by the individuals of the current population. Although the mutation operator alleviates this problem, we cannot guarantee it will find promising regions of the search space within feasible computational time. In this direction, this paper proposes a new geometric dispersion operator that uses multiplicative factors to move individuals to less dense areas of the search space around the target solution before applying semantic genetic operators. Experiments in sixteen datasets show that the results obtained by the proposed operator are statistically significantly better than those produced by GSGP and that the operator does indeed spread the solutions around the target solution.","tags":null,"title":"A Dispersion Operator for Geometric Semantic Genetic Programming","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Luis F. Miranda","Gisele L. Pappa","Fernando E. B. Otero","Ricardo H. C. Takahashi"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"9c062baf376ce87c49b382f53348198f","permalink":"https://luizvbo.github.io/publication/oliveira-2016-reducing/","publishdate":"2020-02-09T16:34:20.272366Z","relpermalink":"/publication/oliveira-2016-reducing/","section":"publication","summary":"Genetic programming approaches are moving from analysing the syntax of individual solutions to look into their semantics. One of the common definitions of the semantic space in the context of symbolic regression is a n-dimensional space, where n corresponds to the number of training examples. In problems where this number is high, the search process can became harder as the number of dimensions increase. Geometric semantic genetic programming (GSGP) explores the semantic space by performing geometric semantic operations—the fitness landscape seen by GSGP is guaranteed to be conic by construction. Intuitively, a lower number of dimensions can make search more feasible in this scenario, decreasing the chances of data overfitting and reducing the number of evaluations required to find a suitable solution. This paper proposes two approaches for dimensionality reduction in GSGP: (i) to apply current instance selection methods as a pre-process step before training points are given to GSGP; (ii) to incorporate instance selection to the evolution of GSGP. Experiments in 15 datasets show that GSGP performance is improved by using instance reduction during the evolution.","tags":null,"title":"Reducing Dimensionality to Improve Search in Semantic Genetic Programming","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Fernando E. B. Otero","Luis F. Miranda","Gisele L. Pappa"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"87ac71225489ba313cfbbb9f055bc611","permalink":"https://luizvbo.github.io/publication/oliveira-2016-revisiting/","publishdate":"2020-02-09T16:34:20.272849Z","relpermalink":"/publication/oliveira-2016-revisiting/","section":"publication","summary":"Sequential Symbolic Regression (SSR) is a technique that recursively induces functions over the error of the current solution, concatenating them in an attempt to reduce the error of the resulting model. As proof of concept, the method was previously evaluated in one-dimensional problems and compared with canonical Genetic Programming (GP) and Geometric Semantic Genetic Programming (GSGP). In this paper we revisit SSR exploring the method behaviour in higher dimensional, larger and more heterogeneous datasets. We discuss the difficulties arising from the application of the method to more complex problems, e.g., overfitting, along with suggestions to overcome them. An experimental analysis was conducted comparing SSR to GP and GSGP, showing SSR solutions are smaller than those generated by the GSGP with similar performance and more accurate than those generated by the canonical GP.","tags":null,"title":"Revisiting the Sequential Symbolic Regression Genetic Programming","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Fernando E. B. Otero","Gisele L. Pappa","Julio Albinati"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"9382253b1f15dff781ad344a5dd73f61","permalink":"https://luizvbo.github.io/publication/oliveira-2015-sequential/","publishdate":"2020-02-09T16:34:20.273267Z","relpermalink":"/publication/oliveira-2015-sequential/","section":"publication","summary":"This chapter describes the Sequential Symbolic Regression (SSR) method, a new strategy for function approximation in symbolic regression. The SSR method is inspired by the sequential covering strategy from machine learning, but instead of sequentially reducing the size of the problem being solved, it sequentially transforms the original problem into potentially simpler problems. This transformation is performed according to the semantic distances between the desired and obtained outputs and a geometric semantic operator. The rationale behind SSR is that, after generating a suboptimal function f via symbolic regression, the output errors can be approximated by another function, in a subsequent iteration. The method was tested in eight polynomial functions, and compared with canonical genetic programming (GP) and geometric semantic genetic programming (SGP). Results showed that SSR significantly outperforms SGP and presents no statistical difference from GP. More importantly, they show the potential of the proposed approach: an effective way of applying geometric semantic operators to combine different (partial) solutions, and at the same time, avoiding the exponential growth problem arising from the use of semantic operators.","tags":null,"title":"Sequential Symbolic Regression with Genetic Programming","type":"publication"},{"authors":["Julio Albinati","Gisele L. Pappa","Fernando E. B. Otero","Luiz Otavio V. B. Oliveira"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"319b2efa3d1bdf473f1bb8683f3795ae","permalink":"https://luizvbo.github.io/publication/albinati-2015-effect/","publishdate":"2020-02-09T16:34:20.274603Z","relpermalink":"/publication/albinati-2015-effect/","section":"publication","summary":"This paper investigates the impact of geometric semantic crossover operators in a wide range of symbolic regression problems. First, it analyses the impact of using Manhattan and Euclidean distance geometric semantic crossovers in the learning process. Then, it proposes two strategies to numerically optimize the crossover mask based on mathematical properties of these operators, instead of simply generating them randomly. An experimental analysis comparing geometric semantic crossovers using Euclidean and Manhattan distances and the proposed strategies is performed in a test bed of twenty datasets. The results show that the use of different distance functions in the semantic geometric crossover has little impact on the test error, and that our optimized crossover masks yield slightly better results. For SGP practitioners, we suggest the use of the semantic crossover based on the Euclidean distance, as it achieved similar results to those obtained by more complex operators.","tags":["Semantic genetic programming; Crossover; Crossover mask optimization"],"title":"The Effect of Distinct Geometric Semantic Crossover Operators in Regression Problems","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Isabela Neves Drummond","Gisele Lobo Pappa"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"3188ffbc85ca090a942c357e8c07571e","permalink":"https://luizvbo.github.io/publication/oliveira-2013-new/","publishdate":"2020-02-09T16:34:20.255097Z","relpermalink":"/publication/oliveira-2013-new/","section":"publication","summary":"This work borrows the traditional Pittsburgh-style representation from Genetic-Based Machine Learning and evaluates its performance in artificial immune systems (AIS) for classification. Our main goal is to select as few instances as possible to represent the data from the training set without losing accuracy. The new representation is tested in a modified version of a clonal selection algorithm, where the antibodies represent lists of prototypes instead of a single one. The generated method, named Clonal Selection Prototypes Generator, was tested in 10 UCI datasets and compared to other seven methods that execute the same task. Results showed that the proposed method is very good at considering a trade-off between the number of prototypes generated and the accuracy of the system.","tags":null,"title":"A new representation for instance-based clonal selection algorithms","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira"],"categories":null,"content":"","date":1346457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1346457600,"objectID":"0ae7695236ee24f48153e36b3e3239b9","permalink":"https://luizvbo.github.io/publication/oliveira-2012-cscdr/","publishdate":"2020-02-09T16:34:20.268982Z","relpermalink":"/publication/oliveira-2012-cscdr/","section":"publication","summary":"","tags":null,"title":"CSCDR: Um Classificador Baseado em Seleção Clonal com Redução de Células de Memória.","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Rodrigo Luiz Mendes Mota","Dante Augusto Couto Barone"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"1600b97371d5277e0036a2d8ed8ab26b","permalink":"https://luizvbo.github.io/publication/oliveira-2012-clonal/","publishdate":"2020-02-09T16:34:20.255658Z","relpermalink":"/publication/oliveira-2012-clonal/","section":"publication","summary":"This study proposes a new algorithm for supervised learning, based on the clonal selection principle exhibited in natural and artificial immune systems. The method, called Clonal Selection Classifier with Data Reduction (CSCDR), utilizes a fitness function based on the number of correct and incorrect pattern classifications made by each antibody. The algorithm tries to maximize this value through clonal selection processes such as mutation, affinity maturation and selection of the best individuals, transforming the training phase in an optimization problem. This leads to antibodies with more representativeness and thus decreases the amount of prototypes generated at the end of the algorithm. Experimental results on benchmark datasets of the UCI machine learning repository demonstrated the effectiveness of the CSCDR algorithm as a classification technique, combined with a considerable data reduction when compared to the results obtained by the well known Artificial Immune Recognition System (AIRS) and the original Clonal Selection Classifier Algorithm (CSCA).","tags":null,"title":"Clonal selection classifier with data reduction: Classification as an optimization task","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Isabela Neves Drummond"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"4d06fcd7159b3cbdf3d738db361ec25e","permalink":"https://luizvbo.github.io/publication/oliveira-2011-real/","publishdate":"2020-02-09T16:34:20.271314Z","relpermalink":"/publication/oliveira-2011-real/","section":"publication","summary":"This work presents a technique based on artificial immune system (AIS) for MR brain image classification. The method is an approach based on real-valued negative selection (RNS) algorithm and the use of a genetic algorithm to find a good combination of the input parameters in the classifier. The tests were carried out on synthetic MR brain images containing multiple sclerosis lesions. Preliminary results obtained shows that our approach is promising. Our implementation is developed in Java using the Weka environment.","tags":null,"title":"Real-Valued Negative Selection (RNS) for MR Brain Image Classification","type":"publication"},{"authors":["Luiz Otavio V. B. Oliveira","Isabela Neves Drummond"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"e03ef07f97fbb385a569bb3842803fbb","permalink":"https://luizvbo.github.io/publication/oliveira-2010-real/","publishdate":"2020-02-09T16:34:20.270761Z","relpermalink":"/publication/oliveira-2010-real/","section":"publication","summary":"This work presents a classification technique based on artificial immune system (AIS). The method consists of a modification of the real-valued negative selection (RNS) algorithm for pattern recognition. Our approach considers a modification in two of the algorithm parameters: the detector radius and the number of detectors for each class. We present an illustrative example. Preliminary results obtained shows that our approach is promising. Our implementation is developed in Java using the Weka environment.","tags":null,"title":"Real-Valued Negative Selection (RNS) for Classification Task","type":"publication"}]